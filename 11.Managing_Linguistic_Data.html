<html lang="ru" dir="ltr" xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=ascii"></meta>
<meta name="generator" content="Docutils 0.12: http://docutils.sourceforge.net/"></meta>
<title>(11)11 Управление лингвистическими данными</title>
<style type="text/css">/* :Author: Edward Loper, James Curran:Copyright: This stylesheet has been placed in the public domain.Stylesheet for use with Docutils.This stylesheet defines new css classes used by NLTK.It uses a Python syntax highlighting scheme that matchesthe colour scheme used by IDLE, which makes it easier forbeginners to check they are typing things in correctly. */
/* Include the standard docutils stylesheet. */
</style>
</head>
<body dir="ltr">
<div class="document" id="managing-linguistic-data">
<span id="chap-data"></span>
<h1 class="title">11. Управление лингвистическими данными</h1>

<!-- -*- mode: rst -*- -->
<!-- -*- mode: rst -*- -->
<!-- CAP abbreviations (map to small caps in LaTeX) -->
<!-- Other candidates for global consistency -->
<!-- PTB removed since it must be indexed -->
<!-- WN removed since it must be indexed -->
<!-- misc & punctuation -->
<!-- cdots was unicode U+22EF but not working -->
<!-- exercise meta-tags -->
<!-- Unicode tests -->
<!-- phonetic -->
<!-- misc -->
<!-- used in Unicode section -->
<!-- arrows -->
<!-- unification stuff -->
<!-- Math & Logic -->
<!-- sets -->
<!-- Greek -->
<!-- Chinese -->
<!-- URLs -->
<!-- Python example - a snippet of code in running text -->
<!-- PlaceHolder example -  something that should be replaced by actual code -->
<!-- Linguistic eXample - cited form in running text -->
<!-- Emphasized (more declarative than just using *) -->
<!-- Grammatical Category - e.g. NP and verb as technical terms
.. role:: gc
   :class: category -->
<!-- Math expression - e.g. especially for variables -->
<!-- Textual Math expression - for words &#39;inside&#39; a math environment -->
<!-- Feature (or attribute) -->
<!-- Raw LaTeX -->
<!-- Raw HTML -->
<!-- Feature-value -->
<!-- Lexemes -->
<!-- Replacements that rely on previous definitions :-) -->
<!-- standard global imports

>>> from __future__ import division
>>> import nltk, re, pprint -->
<!-- TODO: random syllables according to a canon "%s%s" % (random.choice(&#39;ptkbdg&#39;), random.choice(&#39;aeiou&#39;)) -->
<!-- TODO: linguistic annotation -->
<!-- TODO: paradigms -->
<!-- TODO: XML query for access to treebank data (load tree as XML) -->
<!-- TODO: mention GOLD ontology -->
<!-- TODO: building resources for small languages, e.g. spell checker (cf Devonish question) -->
<!-- TODO: part of managing data is tool building...  analyzing needs... -->
<!-- TODO: mention language standards -->
<!-- TODO: instantiating a new corpus reader -->
<p>Структурированные коллекции аннотированных лингвистических данных имеют важное значение в большинстве областей NLP, однако, мы по-прежнему сталкиваемся с многочисленными препятствиями в их использовании.
Цель этой главы - ответить на следующие вопросы:</p>
<ol class="arabic simple">
<li>Как мы разрабатываем новый языковой ресурс и добиваемся того, чтобы его охват, сбалансированность и документация поддерживали широкий спектр применения?</li>
<li>Когда существующие данные находятся в неправильном формате для некоторого инструмента анализа, как мы можем преобразовать его в подходящий формат?</li>
<li>Как документировать существование ресурса, который мы создали, чтобы другие могли легко найти его?</li>
</ol>
<p>Параллельно мы будем изучать конструкцию существующих корпусов, типичный рабочий процесс для создания корпуса, а также жизненный цикл корпуса.
Как и в других главах, будет много примеров из практического опыта управления лингвистическими данными, включая данные, которые были собраны в ходе лингвистической полевой работы, лабораторных работ, а также веб поиска.</p>
<div class="section" id="corpus-structure-a-case-study">
<h1>1 Структура корпуса: изучение примеров</h1>
<p>TIMIT корпус читаемой речи был первой аннотированной базой данных речи, которая была широко распространена, он имел особенно четкую организацию.
TIMIT был разработан консорциумом, включая Texas Instruments и MIT, от которых он получил свое название.
Он был разработан, чтобы предоставить данные для приобретения акустико-фонетических знаний и поддержки разработки и оценки систем автоматического распознавания речи.</p>
<div class="section" id="the-structure-of-timit">
<h2>1.1 Структура TIMIT</h2>
<p>Как и корпус Брауна, который отображает сбалансированный выбор текстовых жанров и источников, TIMIT включает в себя сбалансированный набор диалектов, говорящих и материалов.  Для каждого из восьми диалектных регионов, 50 мужских и женских голосов, имеющих определенный диапазон возрастов и уровня образования, читают по десять тщательно отобранных предложений.  Два предложения, которые читаются всеми голосами, были разработаны, чтобы выявить диалектные вариации:</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(1)</td><td width="15"></td><td><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">a.</td><td width="15"></td><td>she had your dark suit in greasy wash water all year</td></tr></table></p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">b.</td><td width="15"></td><td>don't ask me to carry an oily rag like that
</td></tr></table></p>
</td></tr></table></p>
<p>Остальные предложения были выбраны как фонетически богатые с участием всех фон (звуков) и полного спектра дифон (звуковые биграммы).  Кроме того, конструкция обеспечивает баланс между множественными голосами, читающими то же самое предложение, чтобы позволить проводить сравнения между различными голосами, и широким диапазоном предложений, охватываемых корпус, чтобы получить максимальный охват дифон.  Пять из предложений, прочитанных каждым голосом, также читаются шестью другими голосами (для сопоставимости).  Остальные три предложения, прочитанных каждым голосом, были уникальны для этого голоса (для охвата).</p>
<p>NLTK включает в себя образец из TIMIT корпуса.  Вы можете получить доступ к его документации обычным способом, используя <tt class="doctest"><span class="pre">help(nltk.corpus.timit)</span></tt>.  Выведите на экран <tt class="doctest"><span class="pre">nltk.corpus.timit.fileids()</span></tt>, чтобы просмотреть список из 160 записанных высказываний в образце корпуса.
Каждое имя файла имеет внутреннюю структуру, как показано на <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-timit">1.1</a>.</p>
<span class="target" id="fig-timit"></span><div class="figure" id="fig-timit">
<img alt="../images/timit.png" src="http://www.nltk.org/images/timit.png" style="width:475.2px;height:403.8px">
<p class="caption"><span class="caption-label">Рисунок 1.1:</span> Структура идентификатора TIMIT: Каждая запись помечается с помощью строки, составленной из диалектной области говорящего, пола, идентификатора диктора, типа предложения и идентификатора предложения.</p>
</div>
<p>Каждый элемент имеет фонетическую транскрипцию, которую можно получить с помощью метода <tt class="doctest"><span class="pre">phones()</span></tt>.  Мы можем получить доступ к соответствующим токенам слов обычным способом.  Оба метода обращения допускают необязательный аргумент <tt class="doctest"><span class="pre">offset=True</span></tt>, который включает начальные и конечные смещения соответствующего диапазона в аудиофайле.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; phonetic = nltk.corpus.timit.phones('dr1-fvmh0/sa1')
&gt;&gt;&gt; phonetic
['h#', 'sh', 'iy', 'hv', 'ae', 'dcl', 'y', 'ix', 'dcl', 'd', 'aa', 'kcl',
's', 'ux', 'tcl', 'en', 'gcl', 'g', 'r', 'iy', 's', 'iy', 'w', 'aa',
'sh', 'epi', 'w', 'aa', 'dx', 'ax', 'q', 'ao', 'l', 'y', 'ih', 'ax', 'h#']
&gt;&gt;&gt; nltk.corpus.timit.word_times('dr1-fvmh0/sa1')
[('she', 7812, 10610), ('had', 10610, 14496), ('your', 14496, 15791),
('dark', 15791, 20720), ('suit', 20720, 25647), ('in', 25647, 26906),
('greasy', 26906, 32668), ('wash', 32668, 37890), ('water', 38531, 42417),
('all', 43091, 46052), ('year', 46052, 50522)]</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>В дополнение к этим текстовым данным TIMIT включает в себя лексикон, который обеспечивает каноническое произношение каждого слова, которое можно сравнить с конкретным произношением:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; timitdict = nltk.corpus.timit.transcription_dict()
&gt;&gt;&gt; timitdict['greasy'] + timitdict['wash'] + timitdict['water']
['g', 'r', 'iy1', 's', 'iy', 'w', 'ao1', 'sh', 'w', 'ao1', 't', 'axr']
&gt;&gt;&gt; phonetic[17:30]
['g', 'r', 'iy', 's', 'iy', 'w', 'aa', 'sh', 'epi', 'w', 'aa', 'dx', 'ax']</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Это дает нам ощущение того, что система обработки речи должна сделать для генерирования или распознавания речи в данном конкретном диалекте (New England).  Наконец, TIMIT включает демографические данные о читающих, что позволяет выполнять скурпулезный анализ вокальных, социальных и гендерных характеристик.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; nltk.corpus.timit.spkrinfo('dr1-fvmh0')
SpeakerInfo(id='VMH0', sex='F', dr='1', use='TRN', recdate='03/11/86',
birthdate='01/08/60', ht='5\'05"', race='WHT', edu='BS',
comments='BEST NEW ENGLAND ACCENT SO FAR')</pre>
</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="notable-design-features">
<h2>1.2 Важные конструктивные особенности</h2>
<p>TIMIT иллюстрирует несколько ключевых особенностей конструкции корпуса.
Во-первых, корпус содержит два слоя аннотаций: на фонетическом и орфографическом уровнях.  В общем случае текстовый или речевой корпус может быть аннотирован на разных языковых уровнях, включая морфологический, синтаксический и уровень дискурса.  Более того, даже на данном уровне могут существовать различные схемы маркировки или даже разногласия среди аннотирующих такие, что мы захотим представить несколько версий.
Вторым свойством TIMIT является баланс между множественными измерения вариации относительно покрытия диалектных регионов и дифон.  Включение демографических данных читающих вносит много более независимых переменных, которые могут помочь объяснить вариацию данных и которые облегчают последующее использование корпуса для целей, которые не были предусмотрены при его создании, таких как социолингвистика.
Третьим свойством является то, что существует резкое разделение между первоначальным лингвистическим событием, зафиксированным в виде аудиозаписи, и аннотацией этого события.
То же самое относится и к текстовым корпусам в том смысле, что первоначальный текст, как правило, имеет внешний источник и считается неизменяемым артефактом.  Любые преобразования этого артефакта, которые связаны с человеческим суждением - даже такие простые, как токенизация - подлежат последующим ревизиям, поэтому важно сохранить исходный материал в форме, как можно более близкой к оригиналу.</p>
<span class="target" id="fig-timit-structure"></span><div class="figure" id="fig-timit-structure">
<img alt="../images/timit-structure.png" src="http://www.nltk.org/images/timit-structure.png" style="width:566.4px;height:435.0px">
<p class="caption"><span class="caption-label">Рисунок 1.2:</span> Структура опубликованного TIMIT корпуса: компакт-диск содержит doc, train и test каталоги на верхнем уровне; test и train каталоги каждый имеет 8 подкаталогов, по одному для диалектного региона; каждый из них содержит дополнительные подкаталоги, по одному на читающего; содержимое каталога для читающего женского пола <tt class="doctest"><span class="pre">aks0</span></tt> представлено на рисунке, в нем находятся 10 <tt class="doctest"><span class="pre">wav</span></tt> файлов вместе с текстовой транскрипцией, транскрипцией выровненной по словам и фонетической транскрипцией.</p>
</div>
<p>Четвертой особенностью TIMIT является иерархическая структура корпуса.
С четырьмя файлами на предложение и 10 предложениями для каждого из 500 говорящих он содержит 20.000 файлов.  Они организованы в древовидную структуру, показанную схематично на Рисунке <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-timit-structure">1.2</a>.
На верхнем уровне есть разделение на наборы обучения и тестирования, что мешает его предполагаемому использованию для разработки и оценки статистических моделей.</p>
<p>Наконец, обратите внимание, что, несмотря на то, что TIMIT - это разговорный корпус, его транскрипции и связанные данные являются текстом и могут быть обработаны с использованием программ так же, как и любой другой текстовый корпус.
Таким образом, многие из вычислительных методов, описанных в этой книге, применимы.
Кроме того, обратите внимание, что все типы данных, включенных в TIMIT корпус, попадают в две основные категории: лексика и текст, которые мы обсудим ниже.
Даже демографические данные читающих - это просто еще один экземпляр лексического типа данных.</p>
<p>Это последнее наблюдение становится менее удивительным, если мы примем во внимание, что текст и структуры записей являются основными областями для двух составляющих компьютерной науки управления данными, а именно поиск текста и базы данных.  Примечательной особенностью управления лингвистическими данными является то, что оно, как правило, соединяет оба типа данных вместе и может опираться на результаты и приемы из обоих областей.</p>
</div>
<div class="section" id="fundamental-data-types">
<h2>1.3 Основные типы данных</h2>
<span class="target" id="fig-datatypes"></span><div class="figure" id="fig-datatypes">
<img alt="../images/datatypes.png" src="http://www.nltk.org/images/datatypes.png" style="width:625.8px;height:535.5px">
<p class="caption"><span class="caption-label">Рисунок 1.3:</span> Основные типы лингвистических данных - лексиконы и тексты: несмотря на все их разнообразие, лексиконы имеют структуру записи, в то время как аннотированные тексты имеют временную организацию.</p>
</div>
<p>Несмотря на свою сложность, TIMIT корпус содержит только два основных типа данных, а именно лексиконы и тексты.
Как мы видели в <a class="reference external" href="http://www.nltk.org/book/ch02.html#chap-corpora">2.</a>, большинство лексических ресурсов могут быть представлены с использованием структуры записи, т.е. ключа плюс одно или несколько полей, как показано на <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-datatypes">1.3</a>.  Лексический ресурс может быть обычным словарем или сравнительным списком слов, как показано.  Он также может быть фразовым лексиконом, где ключевым полем является фраза, а не одно слово.
Тезаурус также содержит данные, структурированные как записи, в котором мы ищем записи по неключевым полям, которые соответствуют тематике.
Мы также можем построить специальные табличные данные (известные как парадигмы), чтобы проиллюстрировать контрасты и систематические вариации, как показано на <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-datatypes">1.3</a> для трех глаголов.  Таблица читающих TIMIT корпуса также является своего рода лексиконом.</p>
<p>На самом высоком уровне абстракции текст является представлением реального или вымышленного речевого события и ход времени этого события переносится в сам текст.  Текст может быть небольшим отрывком, таким как одно слово или предложение, или полным рассказом или диалогом.  Он может быть с аннотациями, такими как метки части речи, морфологический анализ, структура дискурса и так далее.
Как мы видели в технике разметки BIO (<a class="reference external" href="http://www.nltk.org/book/ch07.html#chap-chunk">7.</a>), можно представить компоненты более высокого уровня с помощью меток отдельных слов.  Таким образом, абстракция текста показанная на <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-datatypes">1.3</a> является достаточной.</p>
<p>Несмотря на все сложности и идиосинкразии индивидуальных корпусов, в основе своей они представляют собой наборы текстов вместе с данными, имеющими структуру записи.  Содержимое корпуса часто смещено в сторону одного или другого типа данных.
Например, Корпус Брауна содержит 500 текстовых файлов, но мы все же используем таблицу, чтобы связать эти файлы с 15 различными жанрами.  На другом конце спектра, WordNet содержит 117.659 записей синсетов, хотя он включает в себя множество примеров предложений (мини-текстов) для иллюстрации использования слов.  TIMIT является интересной средней точкой в этом спектре, содержащем значительный несвязанный материал обоих типов: тексты и лексиконы.</p>
</div>
</div>
<div class="section" id="the-life-cycle-of-a-corpus">
<span id="sec-life-cycle-of-a-corpus"></span><h1>2 Жизненный цикл корпуса</h1>
<p>Корпуса не рождаются полностью сформированными, но требуют тщательной подготовки и вклада многих людей в течение длительного периода.  Исходные данные должны быть собраны, очищены, документированы, и сохранены в систематическом виде.  Различные слои аннотации могут быть применены, некоторые требующие специальных знаний о морфологии и синтаксиса данного языка.
Успех на этом этапе зависит от создания эффективного рабочего процесса с использованием подходящих инструментов и преобразователей форматов.
Процедуры контроля качества могут быть использованы, чтобы найти несоответствия в аннотациях, а также для обеспечения максимально высокого уровня между согласия между аннотаторами.  Из-за масштаба и сложности задачи подготовка больших корпусов может занять годы и потребовать десятки или сотни человеко-лет усилий.
В этом разделе мы кратко рассмотрим различные этапы жизненного цикла корпуса.</p>
<div class="section" id="three-corpus-creation-scenarios">
<h2>2.1 Три сценария создания корпуса</h2>
<p>В первом типе корпуса его конструкция раскрывается в ходе исследовательской работ создателя. Это образец типичный для традиционной "полевой лингвистики", в которой материал из поисковых сессий анализируется по мере того, как он собирается, а будущие исследования часто основываются на вопросах, которые возникают при анализе сегодняшних. Полученный корпус затем используется в течение последующих нескольких лет исследований и может служить в качестве архивного ресурса неопределенно долгое время.  Компьютеризация является очевидным благом для работы такого типа, примером является популярная программа Shoebox, теперь уже более чем двадцатилетней давности, повторно выпущенная под названием Toolbpx (см. <a class="reference external" href="http://www.nltk.org/book/ch02.html#sec-lexical-resources">4</a>).
Другие программные средства, даже простые текстовые процессоры и электронные таблицы, обычно используются для формирования данных.  В следующем разделе мы рассмотрим, как извлекать данные из этих источников.</p>
<p>Другой сценарий создания корпуса является типичным для экспериментальных исследований, в которых материал, имеющий тщательно продуманную структуру, собирается у различных носителей языка, а затем анализируется, чтобы оценить гипотезу или разработать технологию.
Для таких баз данных стало обычным совместное и повторное использование внутри лаборатории или компании, а часто и публикация для более ширококого круга пользователей. Корпусы такого типа являются основой метода управления исследованием "общая задача", который за последние два десятилетия стал нормой финансируемых государством исследовательских программ в области языковых технологий.
Мы уже встречались со многими такими корпусами в предыдущих главах; мы увидим, как писать программы на Python для реализации различных видов кураторских задач, которые необходимо выполнить до публикации подобных корпусов.</p>
<p>Наконец, есть попытки собрать "эталонный корпус" для конкретного языка, такие как <em>Американский национальный корпус</em> (ANC) и <em>Британский национальный корпус</em> (BNC). Здесь целью было произвести всесторонний учет многих форм, стилей и видов использования языка.
Помимо явного вызова, связанного с масштабом, существует сильная зависимость от автоматических инструментов аннотации и постредактирования для исправления каких-либо ошибок.  Тем не менее, мы можем писать программы для обнаружения и исправления ошибок, а также для анализа корпуса на сбалансированность.</p>
</div>
<div class="section" id="quality-control">
<h2>2.2 Контроль качества</h2>
<p>Хорошие инструменты для автоматической и ручной подготовки данных имеют важное значение.  Однако создание высококачественного корпуса зависит так же от таких приземленных вещей как документация, обучение и рабочий процесс.
Руководства по аннотации определяют задачу и документируют конвенции разметки.  Они могут обновляться на регулярной основе для учета сложных случаев, наряду с новыми правилами, которые разрабатываются для достижения более последовательных аннотаций.  Аннотаторы должны быть обучены процедурам, в том числе методам разрешения ситуаций, не охваченных руководствами.  Последовательность действий должна быть установлена, по возможности, с помощью вспомогательного программного обеспечения, чтобы отслеживать, какие файлы были инициализированы, аннотированы, валидированы, проверены вручную и так далее.  В корпусе может быть несколько слоев аннотации, разработанных различными специалистами.  Случаи неопределенности или разногласия могут требовать вынесения решения.</p>
<p>Большая аннотационная работа может требовать участия нескольких аннотаторов, что создает проблему обеспечения согласованности.
Насколько последовательно группа аннотаторов может выполнять свою работу?
Мы легко можем измерить последовательность, имея часть исходного материала, аннотированного двумя людьми независимо друг от друга.
Это может выявить недостатки в руководствах или различие способностей аннотаторов.
В тех случаях, когда качество имеет первостепенное значение, весь корпус может быть аннотирован дважды, а любые несогласованности разрешаться экспертом.</p>
<p>Считается лучшей практикой сообщать о мере согласия между аннотаторами для корпуса (например, с помощью двойного аннотирования 10% корпуса).  Этот показатель дает полезную информацию о верхней границе ожидаемой результативности любой автоматической системы, которая обучается на этом корпусе.</p>
<div class="caution">
<p class="first admonition-title">Внимание!</p>
<p class="last">Следует проявлять осторожность при интерпретации оценки согласия между аннотаторами, поскольку задачи аннотации сильно различаются по своей сложности.  Например, 90% согласие было бы ужасной оценкой для разметка частей речи, но исключительно высокой оценкой классификации семантических ролей.</p>
</div>
<p><a name="kappa_index_term"></a><span class="termdef">Каппа</span> коэффициент K измеряет согласие между двумя людьми, делающими категориальные суждения, скорректированное на ожидаемое случайное согласие.  Например, предположим, что элемент должен быть аннотирован и четыре варианта кодирования в одинаковой степени вероятны.
Тогда можно было бы ожидать, что два человека, кодирующих случайным образом, будут согласны в 25% случаев.
Таким образом, согласию в размере 25% будет назначен K = 0, а лучшие уровни согласия будут уменьшены соответственно.
Для согласия в 50%, мы получим K = 0,333, так как 50 - это треть пути от 25 до 100.
Существует множество других мер согласия; см. <tt class="doctest"><span class="pre">help(nltk.metrics.agreement</span></tt>).</p>
<span class="target" id="fig-windowdiff"></span><div class="figure" id="fig-windowdiff">
<img alt="../images/windowdiff.png" src="http://www.nltk.org/images/windowdiff.png" style="width:399.75px;height:100.5px">
<p class="caption"><span class="caption-label">Рисунок 2.1</span>: Три сегментации последовательности: маленькие прямоугольники представляют собой символы, слова, предложения, короче говоря, любую последовательность, которая может быть разделена на языковые единицы; S<sub>1</sub> и S<sub>2</sub> находятся в близком согласии, но оба существенно отличаются от S<sub>3</sub>.</p>
</div>
<p>Мы также можем измерить согласие между двумя независимыми сегментациями языкового ввода, например, для токенизации, сегментации на предложения, обнаружения именованных объектов.  В <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-windowdiff">2.1</a> мы видим три возможные сегментации последовательности элементов, которые могли бы быть произведены аннотаторами (или программами).
Хотя ни одна из них не совпадает в точности, S<sub>1</sub> и S<sub>2</sub> находятся в близком согласии, и мы хотели бы иметь подходящую меру их близости.
Windowdiff - это простой алгоритм для оценки согласия двух сегментаций путем пробегания скользящего окна над данными и присуждения частичного кредита для близких промахов.
Если мы предварительно конвертируем наши маркеры в последовательность нулей и единиц для записей, когда за токеном следует граница, мы можем представить сегментации в виде строк и применить windowdiff счетчик.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; s1 = "00000010000000001000000"
&gt;&gt;&gt; s2 = "00000001000000010000000"
&gt;&gt;&gt; s3 = "00010000000000000001000"
&gt;&gt;&gt; nltk.windowdiff(s1, s1, 3)
0.0
&gt;&gt;&gt; nltk.windowdiff(s1, s2, 3)
0.190...
&gt;&gt;&gt; nltk.windowdiff(s2, s3, 3)
0.571...</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>В приведенном выше примере окно имело размер 3.
Вычисление windowdiff скользит с этим окном по парам строк.
В каждой позиции она суммирует число границ, найденных в этом окне, для обеих строк, а затем вычисляет разницу.  Эти различия затем суммируются.
Мы можем увеличить или уменьшить размер окна для управления чувствительностью меры.</p>
</div>
<div class="section" id="curation-vs-evolution">
<h2>2.3 Курирование или эволюция?</h2>
<p>Когда большие корпусы доступны широкому кругу исследователей, последние с растущей вероятностью будут основывать свои изыскания на сбалансированных, сфокусированных подмножествах этих корпусов, разработанных первоначально для совершенно других задач.  Например, база данных Switchboard первоначально была собрана для исследования идентификации говорящего, но с тех пор использовалась в качестве основы для опубликованных исследований в области распознавания речи, произношения слов, небеглости, синтаксиса, интонации и структуры дискурса.
Мотивы для повторного использования лингвистических корпусов включают желание сэкономить время и усилия, желание работать над материалом доступным для репликации другими, а иногда и желание изучать более натуралистические формы языкового поведения, чем было бы возможно в противном случае. Процесс выбора подмножества для такого исследования может считаться нетривиальным вкладом сам по себе.</p>
<p>Помимо выбора соответствующего подмножества корпуса, эта новая работа может включать переформатирование текстового файла (например, преобразование в XML), переименование файлов, повторную токенизацию текста, выбор подмножества данных для обогащения и так далее.
Несколько исследовательских групп могли бы сделать эту работу независимо друг от друга, как показано на рисунке <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-evolution">2.2</a>.  На более позднем этапе, если кто-то захочет объединить источники информации из различных версий, задача, вероятно, будет крайне обременительной.</p>
<span class="target" id="fig-evolution"></span><div class="figure" id="fig-evolution">
<img alt="../images/evolution.png" src="http://www.nltk.org/images/evolution.png" style="width:780.12px;height:222.12px">
<p class="caption"><span class="caption-label">Рисунок 2.2:</span> Эволюция корпус с течением времени: После того, как корпус опубликован, исследовательские группы будут использовать его независимо друг от друга, выбирая и обогащая различные его части; более позднее исследование, которое стремится интегрировать отдельные аннотации стоит перед трудной задачей систематизации аннотаций.</p>
</div>
<p>Задача использования производных корпусов становится еще более сложной из-за отсутствия каких-либо записей о том, как была создана производная версия и какая версия имеет последнюю дату.</p>
<p>Альтернатива этой хаотической ситуации для корпуса заключается в том, чтобы он централизованно курировался, а комитеты экспертов пересматривали его и расширяли через определенные промежутки времени, принимая во внимание материалы, полученные от третьих лиц, периодически публикуя новые релизы.  Печать словарей и национальных корпусов может централизованно курироваться таким образом.  Тем не менее, для большинства корпусов эта модель просто непрактична.</p>
<p>Срединный путь состоит в том, чтобы оригинальная публикация корпуса имела схему для идентификации любого подраздела.  Каждое предложение, дерево или лексическая запись может иметь глобальный уникальный идентификатор, а каждый токен, узел или поле (соответственно) может иметь относительное смещение.
Аннотации, в том числе сегментации, могут ссылаться на источник с помощью этой схемы идентификации (метод, который известен как <a name="standoff_annotation_index_term"></a><span class="termdef">Внешняя аннотация)</span>.
Таким образом, новые аннотации могут распределяться независимо от источника, а несколько независимых аннотаций одного и того же источника, можно сравнивать и обновлять, не касаясь источника.</p>
<p>Если публикация корпуса представлена ​​в нескольких версиях, номер версии или дата может быть частью схемы идентификации.
Таблица соответствия идентификаторов между изданиями корпуса позволит легко обновлять любые внешние аннотации.</p>
<div class="caution">
<p class="first admonition-title">Внимание!</p>
<p class="last">Иногда обновленный корпус содержит ревизии базового материала, которые были внешне аннотированы.  Токены могут быть разделены или объединены, а компоненты могут быть переорганизованы.  В таком корпусе может не быть соответствия один-к-одному между старыми и новыми идентификаторами.  Лучше разорвать внешние аннотации на таких компонентах новой версии, чем молчаливо позволить их идентификаторам ссылаться на несуществующие места в корпусе.</p>
</div>
</div>
</div>
<div class="section" id="acquiring-data">
<h1>3 Получение данных</h1>
<div class="section" id="obtaining-data-from-the-web">
<h2>3.1 Получение данных из сети</h2>
<p>Сеть является богатым источником данных для целей анализа языка.  Мы уже обсуждали методы доступа к отдельным файлам, RSS-каналы и результаты поисковых систем (см. <a class="reference external" href="http://www.nltk.org/book/ch03.html#sec-accessing-text">3.1</a>).  Тем не менее, в некоторых случаях мы хотим получить большое количество веб-текста.</p>
<p>Самый простой подход заключается в получении опубликованного корпуса сетевого текста.  Особая группа по сети как корпусу в ACL (SIGWAC) поддерживает список ресурсов на <tt class="doctest"><span class="pre">http://www.sigwac.org.uk/</span></tt>. Преимуществом использования хорошо определенного сетевого корпуса является то, что он документирован, стабилен и позволяет выполнять воспроизводимые эксперименты.</p>
<p>Если желаемое содержание локализовано на определенном сайте, существует множество утилит для захвата всего доступного содержания сайта, таких как <em>GNU Wget</em><tt class="doctest"><span class="pre">http://www.gnu.org/software/wget/</span></tt>. Для получения максимальной гибкости и контроля, может быть использован сетевой сканер, как, например, <em>Heritrix</em> <tt class="doctest"><span class="pre">http://crawler.archive.org/</span></tt>. Сканеры позволяют осуществлять скурпулезный контроль над тем, где осуществлять поиск, по каким ссылкам следовать и как организовать результаты <a class="reference external" href="http://www.nltk.org/book/bibliography.html#croft2009" id="id1">(Croft, Metzler, &amp; Strohman, 2009)</a>.
Например, если мы хотим составить двуязычный сборник текстов, имеющий соответствующие пары документов на каждом языке, сканер должен определить структуру сайта, чтобы извлечь соответствие между документами, и он должен организовать загруженные страницы таким образом, чтобы отразить это соответствие.  Может быть заманчивым написать свой собственный сетевой сканер, но есть множество подводных камней, связанных с решением таких поблем, как обнаружение MIME типов, преобразование относительных URL-адресов в абсолютные, избежание пападания в циклические структуры ссылок, работа с сетевыми задержками, избежание перегрузки сайта или запрета на доступ к сайту и так далее.</p>
</div>
<div class="section" id="obtaining-data-from-word-processor-files">
<h2>3.2 Получение данных из файлов текстовых редакторов</h2>
<p>Текстовые редакторы часто используются для ручной подготовки текстов и лексиконов в проектах, которые имеют ограниченную вычислительную инфраструктуру.  Такие проекты часто предоставляют шаблоны для ввода данных, хотя текстовые процессоры не гарантируют, что данные правильно структурированы.  Например, каждый текст может иметь название и дату.  Аналогично каждая лексическая запись может иметь определенные обязательные поля.
По мере роста размера и сложности данных все большая часть времени может тратиться на поддержание такой системы.</p>
<p>Как мы можем извлечь содержимое таких файлов, чтобы мы могли работать с ним во внешних программах?
Более того, как мы можем проверить содержимое этих файлов, чтобы помочь авторам создавать хорошо структурированные данные, чтобы качество данных могло быть максимизированно в контексте оригинального авторского процесса?</p>
<p>Возьмем словарь, в котором каждая запись имеет поле части речи, выбранной из 20 возможных вариантов, следующее за полем произношения и отображаемое одиннадцатым жирным шрифтом.  Ни один обычный текстовый процессор не имеет функции поиска или макроса способных проверить, что все поля части речи введены и отображаются правильно.  Эта задача требует утомительной ручной проверки.  Если текстовый процессор позволяет сохранить документ в непатентованном формате, например, в виде простого текста, HTML или XML, мы можем иногда написать программы, чтобы сделать эту проверку автоматически.</p>
<p>Рассмотрим следующий фрагмент лексической записи: "sleep [sli: р] <strong>v.i.</strong> <em>condition of body and mind...</em>".
Мы можем ввести это в MSWord, а затем "Сохранить как веб-страницу", затем проверить полученный HTML-файл:</p>
<pre class="literal-block">
&lt;p class=MsoNormal&gt;sleep
  &lt;span style='mso-spacerun:yes'&gt; &lt;/span&gt;
  [&lt;span class=SpellE&gt;sli:p&lt;/span&gt;]
  &lt;span style='mso-spacerun:yes'&gt; &lt;/span&gt;
  &lt;b&gt;&lt;span style='font-size:11.0pt'&gt;v.i.&lt;/span&gt;&lt;/b&gt;
  &lt;span style='mso-spacerun:yes'&gt; &lt;/span&gt;
  &lt;i&gt;a condition of body and mind ...&lt;o:p&gt;&lt;/o:p&gt;&lt;/i&gt;
&lt;/p&gt;
</pre>
<p>Заметим, что запись представлена в виде HTML абзаца с использованием элемента <tt class="doctest"><span class="pre">&lt;p&gt;</span></tt>, а часть речи появляется внутри элемента <tt class="doctest"><span class="pre">&lt;span style=<span class="pysrc-string">'font-size:11.0pt'</span>&gt;</span></tt>.  Следующая программа определяет набор допустимых частей речи <tt class="doctest"><span class="pre">legal_pos</span></tt>.  Затем она извлекает все содержимое одиннадцатого шрифта из файла <tt class="doctest"><span class="pre">dict.htm</span></tt> и сохраняет его в набор <tt class="doctest"><span class="pre">used_pos</span></tt>.  Заметим, что шаблон поиска содержит подвыражения, заключенное в скобки; только материал, который соответствует этому подвыражения, возвращается функцией <tt class="doctest"><span class="pre">re.findall</span></tt>.  И, наконец, программа строит множество недопустимых частей речи как <tt class="doctest"><span class="pre">used_pos - legal_pos</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])
&gt;&gt;&gt; pattern = re.compile(r"'font-size:11.0pt'&gt;([a-z.]+)&lt;")
&gt;&gt;&gt; document = open("dict.htm", encoding="windows-1252").read()
&gt;&gt;&gt; used_pos = set(re.findall(pattern, document))
&gt;&gt;&gt; illegal_pos = used_pos.difference(legal_pos)
&gt;&gt;&gt; print(list(illegal_pos))
['v.i', 'intrans']</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Эта простая программа представляет собой верхушку айсберга.  Мы можем разработать сложные инструменты для проверки и сообщения об ошибках согласованности файлов текстовых процессоров, чтобы люди, поддерживающие словарь могли исправить исходный файл, <em>используя исходный текстовый процессор</em>.</p>
<p>После того, как мы знаем, что данные правильно отформатированы, мы можем написать другие программы для преобразования данных в другой формат.
Программа в <a class="reference internal" href="http://www.nltk.org/book/ch11.html#code-html2csv">3.1</a> разбирает HTML разметку с использованием <tt class="doctest"><span class="pre">nltk.clean_html()</span></tt>, извлекает слова и их произношения и генерирует вывод в формате "значение отделенное запятой" (CSV).</p>
<span class="target" id="code-html2csv"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
from bs4 import BeautifulSoup

def lexical_data(html_file, encoding="utf-8"):
    SEP = '_ENTRY'
    html = open(html_file, encoding=encoding).read()
    html = re.sub(r'&lt;p', SEP + '&lt;p', html)
    text = BeautifulSoup(html).get_text()
    text = ' '.join(text.split())
    for entry in text.split(SEP):
        if entry.count(' ') &gt; 2:
            yield entry.split(' ', 3)</pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; import csv
&gt;&gt;&gt; writer = csv.writer(open("dict1.csv", "w", encoding="utf-8"))
&gt;&gt;&gt; writer.writerows(lexical_data("dict.htm", encoding="windows-1252"))</pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="http://www.nltk.org/book/pylisting/code_html2csv.py" type="text/x-python"><span class="caption-label">Пример 3.1 (code_html2csv.py)</span></a>: <span class="caption-label">Листинг 3.1</span>: Преобразование HTML созданного с помощью Microsoft Word в CSV.</p></td></tr>
</table></div>
<dl class="docutils">
<dt>with gzip.open(fn+".gz","wb") as f_out:</dt>
<dd>f_out.write(bytes(s, 'UTF-8'))</dd>
</dl>
<div class="note">
<p class="first admonition-title">Замечание</p>
<p class="last">Для более сложной обработки HTML, используйте пакет <em>Beautiful Soup</em> доступный для скачивания со страницы <tt class="doctest"><span class="pre">http://www.crummy.com/software/BeautifulSoup/</span></tt></p>
</div>
</div>
<div class="section" id="obtaining-data-from-spreadsheets-and-databases">
<h2>3.3 Получение данных из электронных таблиц и баз данных</h2>
<p>Электронные таблицы часто используются для создания списков слов или парадигм.
Например, сравнительный словник может быть создан с помощью электронных таблиц со строкой для каждого родственного набора и столбцом для каждого языка (cf. <tt class="doctest"><span class="pre">nltk.corpus.swadesh</span></tt> и <tt class="doctest"><span class="pre">www.rosettaproject.org)</span></tt>.
Большинство программ для создания электронных таблиц может экспортировать свои данные в формате CSV.  Как мы увидим ниже, программы на Python легко могут получить доступ к этим данным с помощью модуля <tt class="doctest"><span class="pre">CSV</span></tt>.</p>
<p>Иногда лексиконы хранятся в полноценной реляционной базе данных.
При правильной нормализации эти базы данных могут обеспечить валидацию данных.  Например, мы можем потребовать, чтобы все части речи брались из определенного словаря, объявив, что поле часть речи является <em>перечислением</em> или внешним ключом, который ссылается на отдельную таблицу частей речи.
Тем не менее, реляционная модель требует, чтобы структура данных (схема) была объявлена заранее, а это идет вразрез с доминирующим подходом к структурированию лингвистических данных, который имеет весьма разведовательный характер.  Поля, которые предполагались обязательными и уникальными, часто оказываются необязательными и допускающими повторения.  Реляционная база данных может учесть это, когда структура данных полностью известна заранее, однако, если это не так, или если почти каждое свойство оказывается необязательным или допускающим повторение, реляционный подход неработоспособен.</p>
<p>Тем не менее, когда наша цель состоит в том, чтобы просто извлечь содержимое из базы данных, достаточно просто сохранить данные из таблицы (или результаты SQL запросов) в формате CSV и загрузить их в нашу программу.  Наша программа может выполнить лингвистически мотивированный запрос, который не может быть выражен в SQL, например, <em>выбрать все слова, которые появляются в примерах предложений, для которых нет словарной статьи</em>.
Для решения этой задачи мы должны были бы извлечь достаточно информации из записи для того, чтобы быть определить ее однозначно, наряду с заголовком и примерами предложений.  Давайте предположим, что эта информация теперь доступна в CSV файле <tt class="doctest"><span class="pre">dict.csv</span></tt>:</p>
<pre class="literal-block">
"sleep","sli:p","v.i","a condition of body and mind ..."
"walk","wo:k","v.intr","progress by lifting and setting down each foot ..."
"wake","weik","intrans","cease to sleep"
</pre>
<p>Теперь мы можем выразить этот запрос, как показано ниже:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; import csv
&gt;&gt;&gt; lexicon = csv.reader(open('dict.csv'))
&gt;&gt;&gt; pairs = [(lexeme, defn) for (lexeme, _, _, defn) in lexicon]
&gt;&gt;&gt; lexemes, defns = zip(*pairs)
&gt;&gt;&gt; defn_words = set(w for defn in defns for w in defn.split())
&gt;&gt;&gt; sorted(defn_words.difference(lexemes))
['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each',
'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Эта информация будет затем направлять текущую работу по обогащению лексикона, работу, которая обновляет содержимое реляционной базы данных.</p>
</div>
<div class="section" id="converting-data-formats">
<h2>3.4 Преобразование форматов данных</h2>
<p>Аннотированные лингвистические данные редко поступают в наиболее удобном формате, часто приходится выполнять различные виды преобразования формата.
Преобразование между кодировками уже обсуждалось (см. <a class="reference external" href="http://www.nltk.org/book/ch03.html#sec-unicode">3.3</a>).  Здесь мы обратим внимание на структуру данных.</p>
<p>В простейшем случае, входные и выходные форматы изоморфны.
Например, мы могли бы преобразовывать лексические данные из формата XML в Toolbox, очень просто транслитерировать записи по одной (<a class="reference internal" href="http://www.nltk.org/book/ch11.html#sec-working-with-xml">4</a>).  Структура данных находит свое отражение в структуре необходимой программы: в <tt class="doctest"><span class="pre"><span class="pysrc-keyword">for</span></span></tt> цикле, содержание которого обрабатывает одну запись.</p>
<p>В другом частом случае выход является переработанной формой входа, такой как инвертированный индекс файла.  Здесь необходимо построить структуру индекса в памяти (см. <a class="reference external" href="http://www.nltk.org/book/ch04.html#code-search-documents">4.8</a>), а затем записать его в файл в нужном формате.
В следующем примере создается индекс, который ставит в соответствие словам из определения словаря соответствующую лексему <a class="reference internal" href="http://www.nltk.org/book/ch11.html#map-word-lexeme"><span id="ref-map-word-lexeme"><img class="callout" alt="[1]" src="http://www.nltk.org/book/callouts/callout1.gif"></span></a> для каждой лексической записи <a class="reference internal" href="http://www.nltk.org/book/ch11.html#lexical-entry"><span id="ref-lexical-entry"><img class="callout" alt="[2]" src="http://www.nltk.org/book/callouts/callout2.gif"></span></a>, предварительно токенизировав текст определения <a class="reference internal" href="http://www.nltk.org/book/ch11.html#definition-text"><span id="ref-definition-text"><img class="callout" alt="[3]" src="http://www.nltk.org/book/callouts/callout3.gif"></span></a> и отбросив короткие слова <a class="reference internal" href="http://www.nltk.org/book/ch11.html#short-words"><span id="ref-short-words"><img class="callout" alt="[4]" src="http://www.nltk.org/book/callouts/callout4.gif"></span></a>.  После того, как индекс был построен, мы открываем файл, а затем перебираем записи индекса, чтобы записать строки в требуемом формате <a class="reference internal" href="http://www.nltk.org/book/ch11.html#required-format"><span id="ref-required-format"><img class="callout" alt="5%" src="http://www.nltk.org/book/callouts/callout5.gif"></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; idx = nltk.Index((defn_word, lexeme) 
...                  for (lexeme, defn) in pairs 
...                  for defn_word in nltk.word_tokenize(defn) 
...                  if len(defn_word) &gt; 3) 
&gt;&gt;&gt; with open("dict.idx", "w") as idx_file:
...     for word in sorted(idx):
...         idx_words = ', '.join(idx[word])
...         idx_line = "{}: {}".format(word, idx_words) 
...         print(idx_line, file=idx_file)</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Полученный файл <tt class="doctest"><span class="pre">dict.idx</span></tt> содержит следующие строки.  (Для большего словаря можно было бы ожидать, что мы увидем несколько лексем для каждой записи индекса.)</p>
<pre class="literal-block">
body: sleep
cease: wake
condition: sleep
down: walk
each: walk
foot: walk
lifting: walk
mind: sleep
progress: walk
setting: walk
sleep: wake
</pre>
<p>В некоторых случаях, входные и выходные данные и состоят из двух или большего числа измерений.
Например, вход может представлять собой набор файлов, каждый из которых содержит один столбец данных о частоте слова.  Требуемый выход может быть двухмерной таблице, в которой исходные столбцы отображаются в виде строк.  В таких случаях мы наполняем внутреннюю структуру данных, заполняя одну колонку за один раз, а затем считываем данные построчно и записываем данные в выходной файл.</p>
<p>В самых неприятных случаях исходный и целевой форматы имеют несколько иной охват области и информация неизбежно теряется при переводе между ними.
Например, мы могли бы объединить несколько файлов Toolbox, чтобы создать единый файл CSV, содержащий сравнительный список слов, теряя все, кроме <tt class="doctest"><span class="pre">\ lx</span></tt> поле входных файлов.
Если файл CSV позже был изменен, будет очень трудно ввести изменения в исходные файлы Toolbox.  Частичное решение этой проблемы "кругооборота" заключается в том, чтобы связать явные идентификаторы каждого языкового объекта и распространять эти идентификаторы с объектами.</p>
</div>
<div class="section" id="deciding-which-layers-of-annotation-to-include">
<h2>3.5 Принятие решения о том, какие слои аннотации включить</h2>
<p>Опубликованные корпусы сильно различаются по богатству информации, которую они содержат.
На минимуме корпус, как правило, содержит последовательность звуков или орфографических символов.  На другом конце спектра корпусе может содержать большое количество информации о синтаксической структуре, морфологии, просодии и смысловом содержании каждого предложения, а также аннотацию дискурсивных отношений или актов диалога.  Эти дополнительные слои аннотации может быть как раз тем, что кому-то нужно для выполнения конкретной задачи анализа данных.
Например, может быть намного легче найти данный лингвистический паттерн, если мы можем осуществить поиск конкретной синтаксической структуры; и может быть проще классифицировать лингвистический паттерн, если смысл каждого слова был отмечен.  Вот некоторые обычно предоставляемые слои аннотации:</p>
<ul class="simple">
<li>Токенизация слов: орфографическая форма текста неоднозначно идентифицирует свои токены.  Токенизированная и нормализованная версия в дополнение к обычной орфографической версии могут быть очень удобным ресурсом.</li>
<li>Сегментация на предложения: как мы видели в <a class="reference external" href="http://www.nltk.org/book/ch03.html#chap-words">3</a>, сегментация на предложения может быть более сложной, чем это кажется.  Поэтому некоторые корпусы использовать явные аннотации, чтобы пометить сегментирование на предложения.</li>
<li>Сегментация на параграфы: параграфы и другие структурные элементы (заголовки, главы и т.д.) могут быть явно помечены.</li>
<li>Часть речи: синтаксическая категория каждого слова в документе.</li>
<li>Синтаксическая структура: древовидная структура, показывающая конституентную структуру предложения.</li>
<li>Поверхностная семантика: аннотации именованных объектов и их соотнесенности, метки семантических ролей.</li>
<li>Диалог и дискурс: метки акта диалога, риторическая структура</li>
</ul>
<p>К сожалению, существует не так много согласованности между существующими корпусами в том, как они представляют аннотации.  Тем не менее, два общих класса представления аннотаций следует различать.  <a name="inline_annotation_index_term"></a><span class="termdef">Внутренняя аннотация</span> изменяет исходный документ путем вставки специальных символов или управляющих последовательностей, которые несут аннотированную информацию.
Например, когда при разметке частей речи документа строка <tt class="doctest"><span class="pre"><span class="pysrc-string">"fly"</span></span></tt> может быть заменена на <tt class="doctest"><span class="pre"><span class="pysrc-string">"fly/NN"</span></span></tt>, чтобы указать, что слово <em>fly</em> существительное в этом контексте.  В противоположность этому <a name="standoff_annotation_index_term_2"></a> <span class="termdef">внешняя аннотация</span> не изменяет исходный документ, но вместо этого создает новый файл, который добавляет аннотационную информацию с помощью указателей, которые ссылаются на исходный документ.  Например, этот новый документ может содержать строку <tt class="doctest"><span class="pre"><span class="pysrc-string">"&lt;token id=8 pos='NN'/&gt;"</span></span></tt>, чтобы указать, что токен 8 является существительным.  (Мы хотели бы быть уверенными, что сама токенизация не подлежит изменению, в противном случае это вызвало бы безмолвную поломку таких ссылок.)</p>
</div>
<div class="section" id="standards-and-tools">
<h2>3.6 Стандарты и инструменты</h2>
<p>Чтобы корпус был полезен широкому кругу, он должен быть доступен в широко поддерживаемом формате.  Тем не менее, передний край исследований NLP зависит от новых видов аннотаций, которые по определению не имеет широкой поддержки.
В целом адекватные инструменты для создания, публикации и использования лингвистических данных не являются широко доступными.  Большинство проектов должны разрабатывать свой собственный набор инструментов для внутреннего использования, который не поможет другим, которые испытывают недостаток необходимых ресурсов.
Кроме того, мы не имеем адекватных, общепринятых стандартов для выражения структуры и содержания корпусов. Без таких стандартов инструменты общего назначения невозможны - хотя в то же время без доступных инструментов, адекватные стандарты вряд ли будут разработаны, приняты и использованы.</p>
<p>Один из возможных ответов на эту ситуацию заключается в том, чтобы продвигаться вперед в разработке общего формата, который достаточно выразителен, чтобы схватить большое разнообразие типов аннотаций (см. примеры в <a class="reference internal" href="http://www.nltk.org/book/ch11.html#sec-further-reading-data">8</a>).
Задача NLP заключается в написании программ, которые могут справиться с общностью таких форматов.
Например, если задача программирования включает в себя древовидные данные, а формат файла разрешает произвольно ориентированные графы, то входные данные должны быть проверены на такие свойства дерева, как наличие корня, связанность и ацикличность.
Если входные файлы содержат другие слои аннотации, программа должна знать, как игнорировать их, когда данные загружаются из файла, и как не испортить или не уничтожить эти слои, когда данные дерева сохраняются обратно в файл.</p>
<p>Другим ответом было написание одноразовых скриптов для манипулирования форматами корпусов; такие скрипты замусорили файловое пространство многих исследователей NLP.
Ридеры корпусов NLTK являются более систематическим подходом, основанным на предположении, что работа по разбору формата корпуса должна выполнять только один раз (для языка программирования).</p>
<span class="target" id="fig-three-layer-arch"></span><div class="figure" id="fig-three-layer-arch">
<img alt="../images/three-layer-arch.png" src="http://www.nltk.org/images/three-layer-arch.png" style="width:538.8px;height:241.5px">
<p class="caption"><span class="caption-label">Рисунок 3.2:</span> Общий формат против общего интерфейса</p>
</div>
<p>Вместо того, чтобы ставить акцент на общем формате, мы считаем более перспективным развивать общий интерфейс (см. <tt class="doctest"><span class="pre">nltk.corpus</span></tt>).  Рассмотрим случай treebanks корпуса, который является важным типом корпуса для работы в NLP. Есть много способов хранить дерево фразовой структуры в файле.
Мы можем использовать вложенные скобки, или вложенные элементы XML, или обозначение зависимости с помощью пары (идентификатор ребенка, идентификатор родителя) в каждой строке, или XML-версии обозначения зависимости и т.д. Однако в каждом конкретном случае логическая структура почти одна и та же.
Намного проще разработать общий интерфейс, который позволяет разработчикам приложений писать код для доступа к данным дерева с использованием таких методов, как <tt class="doctest"><span class="pre">children()</span></tt>, <tt class="doctest"><span class="pre">leaves()</span></tt>, <tt class="doctest"><span class="pre">depth()</span></tt> и так далее.
Обратите внимание, что этот подход следует общепринятой практике в ​​рамках информатики, а именно абстрактные типы данных, объектно-ориентированное проектирование и трехслойная архитектура (<a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-three-layer-arch">3.2</a>).
Последний из них - из мира реляционных баз данных - позволяет приложениям для конечных пользователей использовать общую модель ("реляционную модель") и общий язык (SQL), чтобы абстрагироваться от идиосинкразии файлового хранилища, а также позволяет осуществлять изменения в файловой системе не тревожа приложений для конечных пользователей.
Таким же образом общий интерфейс корпуса отделяет прикладные программы от форматов данных.</p>
<p>В этом контексте при создании нового корпуса, предназначенного для распространения среди широкого круга пользователей, целесообразно использовать существующий широко используемый формат везде, где это возможно.
Если это невозможно, корпус может сопровождаться программным обеспечением - таким как модуль <tt class="doctest"><span class="pre">nltk.corpus</span></tt> - который поддерживает существующие методы интерфейса.</p>
</div>
<div class="section" id="special-considerations-when-working-with-endangered-languages">
<h2>3.7 Особые соображения при работе с вымирающими языками</h2>
<p>Важность языка для науки и искусства равна по своему значению культурным сокровищам, воплощенным в языке.
Каждый из примерно 7.000 человеческих языков богат в различных отношениях, своей устной историей и легендами создания, грамматическими конструкциями и самими словами, нюансами их значений.
Находящиеся под угрозой исчезновения реликтовые культуры имеют слова, чтобы отличить подвиды растений согласно терапевтическому применению, которое неизвестно науке.  Языки развиваются с течением времени, по мере того, как они вступают в контакт друг с другом, и каждый из них дает уникальное видение предыстории человека.
Во многих частях мира небольшие языковые вариации от одного города к другому складываются в совершенно другой язык в получасе езды на машине.  За его захватывающую сложность и многообразие человеческий язык можно назвать красочным гобеленом, простирающимся во времени и пространстве.</p>
<p>Тем не менее, большинство языков мира находятся на грани вымирания.
В ответ на этот вызов многие лингвисты усердно работают над документированием языков, созданием полных записей этого важного аспекта языкового наследия мира.
Что может NLP предложить, чтобы помочь этим усилиям?  Разработка разметчиков, анализаторов, распознавателей именованных объектов и т.д. не является первоочередной задачей, в любом случае, как правило, для разработки таких инструментов пока недостаточно данных.  Вместо этого наиболее часто озвучивается потребность иметь лучшие инструменты для сбора и курирования данных с акцентом на текстах и ​​словарях.</p>
<p>Внешне кажется простым делом начать собирать тексты на исчезающем языке.  Но, даже если мы будем игнорировать спорные вопросы, такие как, кто владеет текстами и восприятиями, окружающими культурные знания, содержащиеся в текстах, есть очевидный практический вопрос транскрипции.
Большинство языков не имеют стандартной орфографии.  Если язык не имеет литературной традиции, соглашения об орфографии и пунктуации не являются хорошо установившимися.  Поэтому обычной практикой является создание лексикона вместе со сборником текстов, постоянно обновляя лексикон по мере возникновения новых слов в текстах.  Эта работа может быть сделано с помощью текстового процессора (для текстов) и электронных таблиц (для лексикона).
А еще лучше с помощью свободного лингвистического программного обеспечения SIL Toolbox и Fieldworks, которое обеспечивают всестороннюю поддержку интегрированного создания текстов и словарей.</p>
<p>Когда носители данного языка научены вводить тексты самостоятельно, обще распространенной трудностью является забота о правильном написании.  Использование лексикона очень помогает этому процессу, но мы должны иметь методов поиска, которые не предполагают, что кто-то может определить нормальную форму произвольного слова.  Проблема может быть острой для языков, имеющих сложную морфологию, которая включает префиксы.  В таких случаях помогают семантические пометки лексических единиц и возможность поиска по семантической области или глоссарию.</p>
<p>Возможность поиска по произносительному подобию также оказывает большую помощь.
Вот простая демонстрация того, как сделать это.
Первый шаг заключается в определении смешиваемых последовательностей букв и сопоставлении сложных версий и более простых версий.  Мы могли бы также заметить, что относительный порядок букв внутри кластера согласных является источником орфографических ошибок, поэтому мы нормализуем порядок согласных звуков.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; mappings = [('ph', 'f'), ('ght', 't'), ('^kn', 'n'), ('qu', 'kw'),
...             ('[aeiou]+', 'a'), (r'(.)\1', r'\1')]
&gt;&gt;&gt; def signature(word):
...     for patt, repl in mappings:
...         word = re.sub(patt, repl, word)
...     pieces = re.findall('[^aeiou]+', word)
...     return ''.join(char for piece in pieces for char in sorted(piece))[:8]
&gt;&gt;&gt; signature('illefent')
'lfnt'
&gt;&gt;&gt; signature('ebsekwieous')
'bskws'
&gt;&gt;&gt; signature('nuculerr')
'nclr'</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Далее мы создаем отображение множества ключей на множестве слов для всех слов в нашем лексиконе.  Мы можем использовать это, чтобы получить корректировки кандидатов для данного входного слова (но мы должны сначала вычислить ключ этого слова).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; signatures = nltk.Index((signature(w), w) for w in nltk.corpus.words.words())
&gt;&gt;&gt; signatures[signature('nuculerr')]
['anicular', 'inocular', 'nucellar', 'nuclear', 'unicolor', 'uniocular', 'unocular']</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>И, наконец, мы должны ранжировать результаты по сходству с оригинальным словом.
Это делается с помощью функции <tt class="doctest"><span class="pre">rank()</span></tt>.  Единственная оставшаяся функция предоставляет простой интерфейс для пользователя:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; def rank(word, wordlist):
...     ranked = sorted((nltk.edit_distance(word, w), w) for w in wordlist)
...     return [word for (_, word) in ranked]
&gt;&gt;&gt; def fuzzy_spell(word):
...     sig = signature(word)
...     if sig in signatures:
...         return rank(word, signatures[sig])
...     else:
...         return []
&gt;&gt;&gt; fuzzy_spell('illefent')
['olefiant', 'elephant', 'oliphant', 'elephanta']
&gt;&gt;&gt; fuzzy_spell('ebsekwieous')
['obsequious']
&gt;&gt;&gt; fuzzy_spell('nucular')
['anicular', 'inocular', 'nucellar', 'nuclear', 'unocular', 'uniocular', 'unicolor']</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Это только один пример того, когда простая программа может облегчить доступ к лексическим данным в условиях, когда письменная система языка не может быть стандартизирована или когда пользователи языка могут не иметь хорошего уровня владения орфографией.  Другие простые приложения НЛП в этой области включают: построение индексов для облегчения доступа к данным, формирование списков слов из текстов, поиск примеров использования слова при построении лексикона, обнаружение распространенных или исключительных паттернов в плохо понятых данных, а также выполнение специализированной проверки данных, созданных с использованием различных лингвистических программных средств.  Мы вернемся к последним в <a class="reference internal" href="http://www.nltk.org/book/ch11.html#sec-working-with-toolbox-data">5</a>.</p>
</div>
</div>
<div class="section" id="working-with-xml">
<span id="sec-working-with-xml"></span><h1>4 Работа с XML</h1>
<p>Расширяемый язык разметки (Extensible Markup Language (XML)) обеспечивает основу для разработки предметно-ориентированных языков разметки.  Иногда он используется для представления аннотированного текста и лексических ресурсов.  В отличие от HTML с его предопределенными метками XML позволяет нам создать наши собственные метки.  В отличие от базы данных XML позволяет создавать данные без предварительного определения их структуры, а это позволяет нам иметь необязательные и повторяющиеся элементы.  В этом разделе мы кратко рассмотрим некоторые особенности XML, которые имеют отношение к представлению лингвистических данных, и покажем, как обращаться к данным, хранящимся в файлах XML с использованием программ на Python.</p>
<div class="section" id="using-xml-for-linguistic-structures">
<h2>4.1 Использование XML для языковых структур</h2>
<p>Благодаря своей гибкости и расширяемости XML является естественным выбором для представления лингвистических структур.  Вот пример простой лексической записи.</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(2)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;
  &lt;headword&gt;whale&lt;/headword&gt;
  &lt;pos&gt;noun&lt;/pos&gt;
  &lt;gloss&gt;any of the larger cetacean mammals having a streamlined
    body and breathing through a blowhole on the head&lt;/gloss&gt;
&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>Она состоит из серии XML тегов в угловых скобках.
Каждый открывающий тег, как <tt class="doctest"><span class="pre">&lt;gloss&gt;</span></tt> сочетается с закрывающим тегом, например <tt class="doctest"><span class="pre">&lt;/gloss&gt;</span></tt>; вместе они представляют собой <a name="xml_element_index_term"></a><span class="termdef">XML-элемент</span>.
Приведенный выше пример был отформатирован красиво с помощью пробелов, но он так же мог быть расположен на одной длинной строке.  Наш подход к обработке XML, как правило, не чувствителен к пробелам.  Для того, чтобы XML был <a name="well_formed_index_term"></a><span class="termdef">хорошо сформирован</span>, все открывающие теги должны иметь соответствующие закрывающие теги на том же уровне вложенности (то есть документ XML должен быть правильно сформированным деревом).</p>
<p>XML позволяет повторять элементы, например, добавить еще одно поле глоссария, как мы увидим ниже.
Мы будем использовать различные пропуски, чтобы подчеркнуть, что макет не имеет значения.</p>
<p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(3)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;&lt;headword&gt;whale&lt;/headword&gt;&lt;pos&gt;noun&lt;/pos&gt;&lt;gloss&gt;any of the
larger cetacean mammals having a streamlined body and breathing
through a blowhole on the head&lt;/gloss&gt;&lt;gloss&gt;a very large person;
impressive in size or qualities&lt;/gloss&gt;&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>Следующим шагом может заключаться в том, чтобы связать наш лексикон с каким-то внешним ресурсом, таким как WordNet, с помощью внешних идентификаторов.  В <a class="reference internal" href="http://www.nltk.org/book/ch11.html#ex-xml-nested">(4)</a> мы группируем глоссу и идентификатор синсета внутри нового элемента, который мы назвали "sense".</p>
<span class="target" id="ex-xml-nested"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(4)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;
  &lt;headword&gt;whale&lt;/headword&gt;
  &lt;pos&gt;noun&lt;/pos&gt;
  &lt;sense&gt;
    &lt;gloss&gt;any of the larger cetacean mammals having a streamlined
      body and breathing through a blowhole on the head&lt;/gloss&gt;
    &lt;synset&gt;whale.n.02&lt;/synset&gt;
  &lt;/sense&gt;
  &lt;sense&gt;
    &lt;gloss&gt;a very large person; impressive in size or qualities&lt;/gloss&gt;
    &lt;synset&gt;giant.n.04&lt;/synset&gt;
  &lt;/sense&gt;
&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>В качестве альтернативы мы могли бы представить идентификатор синсета с помощью <a name="xml_attribute_index_term"></a> <span class="termdef">XML атрибута</span> без необходимости в каких-либо вложенных структурах, как в <a class="reference internal" href="http://www.nltk.org/book/ch11.html#ex-xml-attribute">(5)</a>.</p>
<span class="target" id="ex-xml-attribute"></span><p><table border="0" cellpadding="0" cellspacing="0" class="example">
  <tr valign="top"><td width="30" align="right">(5)</td><td width="15"></td><td><pre class="literal-block">
&lt;entry&gt;
  &lt;headword&gt;whale&lt;/headword&gt;
  &lt;pos&gt;noun&lt;/pos&gt;
  &lt;gloss synset="whale.n.02"&gt;any of the larger cetacean mammals having
      a streamlined body and breathing through a blowhole on the head&lt;/gloss&gt;
  &lt;gloss synset="giant.n.04"&gt;a very large person; impressive in size or
      qualities&lt;/gloss&gt;
&lt;/entry&gt;
</pre>
</td></tr></table></p>
<p>Это иллюстрирует некоторые аспекты гибкости XML.  Если примеры кажутся несколько произвольными, то это потому что так оно и есть!  Следуя правилам XML мы можем изобретать новые имена атрибутов и вкладывать их так глубоко, как нам хочется.  Мы можем повторять элементы, забывать их, размещать их каждый раз в другом порядке.  Мы можем иметь поля, наличие которых зависит от значения какого-либо другого поля, например , если часть речи "verb", тогда запись может иметь элемент <tt class="doctest"><span class="pre">past_tense</span></tt> для хранения прошедшего времени глагола, но если часть речи "noun", то <tt class="doctest"><span class="pre">past_tense</span></tt> элемент не допускается.  Чтобы навести какой-то порядок во всей этой свободе, мы можем ограничить структуру файла XML с помощью "схемы", которая является заявлением сродни контекстно-свободной грамматике.  Существуют инструменты для тестирования <a name="validity_index_term"></a><span class="termdef">валидности</span> XML файла в соответствии со схемой.</p>
</div>
<div class="section" id="the-role-of-xml">
<h2>4.2 Роль XML</h2>
<p>Мы можем использовать XML для представления многих видов лингвистической информации.
Тем не менее, эта гибкость дается определенной ценой.  Каждый раз, когда мы вводим усложнение, например, разрешая элементу быть необязательным или повторяться, мы создаем дополнительную работу для любой программы, которая обращается к этим данным.  Мы также делаем более трудной проверку действительности данных или запрос данных с использованием одного из языков XML-запросов.</p>
<p>Таким образом, использование XML для представления лингвистических структур не может магическим образом решить проблему моделирования данных.  Нам все равно необходимо решить, как структурировать данные, затем определить эту структуру с помощью схемы, а затем писать программы для чтения и записи в данном формате и конвертации его в другие форматы.
Кроме того, мы все равно должны следовать некоторым стандартным принципам, касающимся нормализации данных.  Целесообразно избегать дублирования одной и той же информации, чтобы в конечном итоге не столкнуться с несовместимостью данных, когда будет изменена только одна копия.  Например, перекрестная ссылка, которая был представлена в виде <tt class="doctest"><span class="pre">&lt;xref&gt; headword&lt;/xref&gt;</span></tt> будет дублировать хранилище headword какой-либо другой лексической записи, и эта ссылка перестанет работать, если копия строки в другом месте будет изменена.
Экзистенциальные зависимости между типами информации должны быть смоделированы, чтобы мы не могли создать элементы без дома.
Например, если определения смысла не могут существовать независимо от лексической записи, то элемент <tt class="doctest"><span class="pre">смысл</span></tt> может быть вложен только внутрь элемента <tt class="doctest"><span class="pre">запись</span></tt>.  Отношения многие-ко-многим должны быть выведены из иерархических структур.  Например, если слово может иметь множество соответствующих смыслов, а смысл может иметь несколько соответствующих слов, тогда и слова, и смыслы должны быть перечислены по отдельности, так же как и список пар (слово, смысл).  Эта сложная структура может даже быть разбита на три отдельных файла XML.</p>
<p>Как мы можем видеть, хотя XML предоставляет нам удобный формат, сопровождаемый обширной коллекцией инструментов, он не является панацеей.</p>
</div>
<div class="section" id="the-elementtree-interface">
<h2>4.3 Интерфейс ElementTree</h2>
<p>Python модуль ElementTree обеспечивает удобный способ доступа к данным, хранящимся в файлах XML.  ElementTree является частью стандартной библиотеки языка Python (с Python 2.5), а также предоставляется в рамках NLTK в случае, если вы используете Python 2.4.</p>
<p>Проиллюстрируем использование ElementTree с помощью коллекции пьес Шекспира, которые были отформатированы с помощью XML. Давайте загрузим файл XML и исследуем его содержимое, сперва начало файла <a class="reference internal" href="http://www.nltk.org/book/ch11.html#top-of-file"><span id="ref-top-of-file"><img class="callout" alt="[1]" src="http://www.nltk.org/book/callouts/callout1.gif"></span></a>, где мы видим некоторые XML заголовки и имя схемы под названием <tt class="doctest"><span class="pre">play.dtd</span></tt>, за которым следует <a name="root_element_index_term"></a><span class="termdef">корневой элемент</span> <tt class="doctest"><span class="pre">PLAY</span></tt>. Мы подбираем его снова в начале 1-го акта <a class="reference internal" href="http://www.nltk.org/book/ch11.html#start-act-one"><span id="ref-start-act-one"><img class="callout" alt="[2]" src="http://www.nltk.org/book/callouts/callout2.gif"></span></a>.
(Некоторые пустые строки были опущены в выводе.)</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; merchant_file = nltk.data.find('corpora/shakespeare/merchant.xml')
&gt;&gt;&gt; raw = open(merchant_file).read()
&gt;&gt;&gt; print(raw[:163]) 
&lt;?xml version="1.0"?&gt;
&lt;?xml-stylesheet type="text/css" href="shakes.css"?&gt;
&lt;!-- &lt;!DOCTYPE PLAY SYSTEM "play.dtd"&gt; --&gt;
&lt;PLAY&gt;
&lt;TITLE&gt;The Merchant of Venice&lt;/TITLE&gt;
&gt;&gt;&gt; print(raw[1789:2006]) 
&lt;TITLE&gt;ACT I&lt;/TITLE&gt;
&lt;SCENE&gt;&lt;TITLE&gt;SCENE I.  Venice. A street.&lt;/TITLE&gt;
&lt;STAGEDIR&gt;Enter ANTONIO, SALARINO, and SALANIO&lt;/STAGEDIR&gt;
&lt;SPEECH&gt;
&lt;SPEAKER&gt;ANTONIO&lt;/SPEAKER&gt;
&lt;LINE&gt;In sooth, I know not why I am so sad:&lt;/LINE&gt;</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Мы только что обратились к данным XML как строке.  Как мы видим, строка в начале первого акта содержит XML-теги для заголовка, сцены, ремарок и так далее.</p>
<p>Следующим шагом будет обработать содержимое файла как структурированные данные XML с помощью <tt class="doctest"><span class="pre">ElementTree</span></tt>.  Мы обрабатываем файл (многострочный текст) и строим дерево, поэтому не удивительно, что метод называется <tt class="doctest"><span class="pre">parse</span></tt> <a class="reference internal" href="http://www.nltk.org/book/ch11.html#xml-parse"><span id="ref-xml-parse"><img class="callout" alt="[1]" src="http://www.nltk.org/book/callouts/callout1.gif"></span></a>.
Переменная <tt class="doctest"><span class="pre">merchant</span></tt> содержит XML-элемент <tt class="doctest"><span class="pre">PLAY</span></tt> <a class="reference internal" href="http://www.nltk.org/book/ch11.html#element-play"><span id="ref-element-play"><img class="callout" alt="[2]" src="http://www.nltk.org/book/callouts/callout2.gif"></span></a>.
Этот элемент имеет внутреннюю структуру; мы можем использовать индекс, чтобы получить его первого ребенка, элемент<tt class="doctest"><span class="pre">TITLE</span></tt> <a class="reference internal" href="http://www.nltk.org/book/ch11.html#element-title"><span id="ref-element-title"><img class="callout" alt="[3]" src="http://www.nltk.org/book/callouts/callout3.gif"></span></a>.
Мы также можем увидеть текстовое содержимое этого элемента, название пьесы <a class="reference internal" href="http://www.nltk.org/book/ch11.html#element-text"><span id="ref-element-text"><img class="callout" alt="[4]" src="http://www.nltk.org/book/callouts/callout4.gif"></span></a>.
Для того, чтобы получить список всех дочерних элементов, мы используем метод <tt class="doctest"><span class="pre">getchildren()</span></tt> <a class="reference internal" href="http://www.nltk.org/book/ch11.html#getchildren-method"><span id="ref-getchildren-method"><img class="callout" alt="5%" src="http://www.nltk.org/book/callouts/callout5.gif"></span></a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; from xml.etree.ElementTree import ElementTree
&gt;&gt;&gt; merchant = ElementTree().parse(merchant_file) 
&gt;&gt;&gt; merchant
&lt;Element 'PLAY' at 0x10ac43d18&gt; # [_element-play]
&gt;&gt;&gt; merchant[0]
&lt;Element 'TITLE' at 0x10ac43c28&gt; # [_element-title]
&gt;&gt;&gt; merchant[0].text
'The Merchant of Venice' # [_element-text]
&gt;&gt;&gt; merchant.getchildren() 
[&lt;Element 'TITLE' at 0x10ac43c28&gt;, &lt;Element 'PERSONAE' at 0x10ac43bd8&gt;,
&lt;Element 'SCNDESCR' at 0x10b067f98&gt;, &lt;Element 'PLAYSUBT' at 0x10af37048&gt;,
&lt;Element 'ACT' at 0x10af37098&gt;, &lt;Element 'ACT' at 0x10b936368&gt;,
&lt;Element 'ACT' at 0x10b934b88&gt;, &lt;Element 'ACT' at 0x10cfd8188&gt;,
&lt;Element 'ACT' at 0x10cfadb38&gt;]</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Пьеса состоит из названия, персон, описания сцены, подзаголовка и пяти актов.
Каждый акт имеет название и несколько сцен, а каждая сцена состоит из речей, которые составлены из строк - структура с четырьмя уровнями вложенности.  Давайте покопаемсе в четвертом акте:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; merchant[-2][0].text
'ACT IV'
&gt;&gt;&gt; merchant[-2][1]
&lt;Element 'SCENE' at 0x10cfd8228&gt;
&gt;&gt;&gt; merchant[-2][1][0].text
'SCENE I.  Venice. A court of justice.'
&gt;&gt;&gt; merchant[-2][1][54]
&lt;Element 'SPEECH' at 0x10cfb02c8&gt;
&gt;&gt;&gt; merchant[-2][1][54][0]
&lt;Element 'SPEAKER' at 0x10cfb0318&gt;
&gt;&gt;&gt; merchant[-2][1][54][0].text
'PORTIA'
&gt;&gt;&gt; merchant[-2][1][54][1]
&lt;Element 'LINE' at 0x10cfb0368&gt;
&gt;&gt;&gt; merchant[-2][1][54][1].text
"The quality of mercy is not strain'd,"</pre>
</td>
</tr></table></td></tr>
</table></div>
<div class="note">
<p class="first admonition-title">Замечание</p>
<p class="last"><strong>Ваша очередь:</strong> 
Повторите некоторые из описанных выше методов для одной из других пьес Шекспира, включенных в корпус, таких как <span class="emphasis">Ромео и Джульетта</span> или <span class="emphasis">Макбет</span>; см. список в <tt class="doctest"><span class="pre">nltk.corpus.shakespeare.fileids()</span></tt>.</p>
</div>
<p>Хотя мы можем получить доступ ко всему дереву таким образом, более удобно искать подэлементы по их именам.  Напомним, что на верхнем уровне элементы имеют несколько типов.  Мы можем перебрать только интересующие нас типы (например, акты) с помощью метода <tt class="doctest"><span class="pre">merchant.findall(<span class="pysrc-string">'ACT'</span>)</span></tt>.  Вот пример выполнения поиска по конкретной метке на каждом уровне вложенности:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; for i, act in enumerate(merchant.findall('ACT')):
...     for j, scene in enumerate(act.findall('SCENE')):
...         for k, speech in enumerate(scene.findall('SPEECH')):
...             for line in speech.findall('LINE'):
...                 if 'music' in str(line.text):
...                     print("Act %d Scene %d Speech %d: %s" % (i+1, j+1, k+1, line.text))
Act 3 Scene 2 Speech 9: Let music sound while he doth make his choice;
Act 3 Scene 2 Speech 9: Fading in music: that the comparison
Act 3 Scene 2 Speech 9: And what is music then? Then music is
Act 5 Scene 1 Speech 23: And bring your music forth into the air.
Act 5 Scene 1 Speech 23: Here will we sit and let the sounds of music
Act 5 Scene 1 Speech 23: And draw her home with music.
Act 5 Scene 1 Speech 24: I am never merry when I hear sweet music.
Act 5 Scene 1 Speech 25: Or any air of music touch their ears,
Act 5 Scene 1 Speech 25: By the sweet power of music: therefore the poet
Act 5 Scene 1 Speech 25: But music for the time doth change his nature.
Act 5 Scene 1 Speech 25: The man that hath no music in himself,
Act 5 Scene 1 Speech 25: Let no such man be trusted. Mark the music.
Act 5 Scene 1 Speech 29: It is your music, madam, of the house.
Act 5 Scene 1 Speech 32: No better a musician than the wren.</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Вместо перемещения на каждом шаге вниз по иерархии мы можем искать конкретные встроенные элементы.  Например, давайте рассмотрим последовательность говорящих.
Мы можем использовать распределение частот, чтобы увидеть, кто больше всех говорит:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; speaker_seq = [s.text for s in merchant.findall('ACT/SCENE/SPEECH/SPEAKER')]
&gt;&gt;&gt; speaker_freq = Counter(speaker_seq)
&gt;&gt;&gt; top5 = speaker_freq.most_common(5)
&gt;&gt;&gt; top5
[('PORTIA', 117), ('SHYLOCK', 79), ('BASSANIO', 73),
('GRATIANO', 48), ('LORENZO', 47)]</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Мы можем также искать закономерности в том, кто следует за кем в диалогах.
Так как в пьесе присутствует 23 говорящих, мы должны сначала уменьшить "словарь" до приемлемого размера с помощью метода, описанного в <a class="reference external" href="http://www.nltk.org/book/ch05.html#sec-dictionaries">3</a>.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; from collections import defaultdict
&gt;&gt;&gt; abbreviate = defaultdict(lambda: 'OTH')
&gt;&gt;&gt; for speaker, _ in top5:
...     abbreviate[speaker] = speaker[:4]
...
&gt;&gt;&gt; speaker_seq2 = [abbreviate[speaker] for speaker in speaker_seq]
&gt;&gt;&gt; cfd = nltk.ConditionalFreqDist(nltk.bigrams(speaker_seq2))
&gt;&gt;&gt; cfd.tabulate()
     ANTO BASS GRAT  OTH PORT SHYL
ANTO    0   11    4   11    9   12
BASS   10    0   11   10   26   16
GRAT    6    8    0   19    9    5
 OTH    8   16   18  153   52   25
PORT    7   23   13   53    0   21
SHYL   15   15    2   26   21    0</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Игнорируя записи обмена репликами между всеми, кроме 5 первых (помеченных <tt class="doctest"><span class="pre">OTH)</span></tt>, наибольшее значение предполагает, что Порция и Бассанио имеют наиболее частые взаимодействия.</p>
</div>
<div class="section" id="using-elementtree-for-accessing-toolbox-data">
<h2>4.4 Использование ElementTree для доступа к данным Toolbox</h2>
<p>В <a class="reference external" href="http://www.nltk.org/book/ch02.html#sec-lexical-resources">4</a> мы увидели простой интерфейс для доступа к данным Toolbox, популярному и весьма устоявшемуся формату, используемому лингвистами для управления данными.
В этом разделе мы рассмотрим различные методы управления данными Toolbox способами, которые не поддерживаются программным обеспечением Toolbox.  Методы, которые мы обсудим, могли быть применены к другим данным, имеющим структуру записей, независимо от фактического формата файла.</p>
<p>Мы можем использовать метод <tt class="doctest"><span class="pre">toolbox.xml()</span></tt>, чтобы получить доступ к файлу Toolbox и загрузить его в объект <tt class="doctest"><span class="pre">ElementTree</span></tt>.  Этот файл содержит словарь для языка Ротокас Папуа Новой Гвинеи.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; from nltk.corpus import toolbox
&gt;&gt;&gt; lexicon = toolbox.xml('rotokas.dic')</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Есть два способа получить доступ к содержимому объекта лексикона, по  индексам и по путям.  Индексы используют знакомый синтаксис, так <tt class="doctest"><span class="pre">lexicon[3]</span></tt> возвращает запись номер 3 (которая, на самом деле, четвертая запись, считая с нуля); <tt class="doctest"><span class="pre">lexicon[3][0]</span></tt> возвращает первое поле:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; lexicon[3][0]
&lt;Element 'lx' at 0x10b2f6958&gt;
&gt;&gt;&gt; lexicon[3][0].tag
'lx'
&gt;&gt;&gt; lexicon[3][0].text
'kaa'</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Второй способ получить доступ к содержимому объекта лексикона использует пути.  Лексикон представляет собой серию объектов <tt class="doctest"><span class="pre">record</span></tt>, каждый из которых содержит ряд объектов полей, таких как <tt class="doctest"><span class="pre">lx</span></tt> и <tt class="doctest"><span class="pre">ps</span></tt>.  Мы можем легко обратиться ко всем лексемам, используя путь <tt class="doctest"><span class="pre">record/lx</span></tt>.
Здесь мы используем функцию <tt class="doctest"><span class="pre">findall()</span></tt> для поиска каких-либо совпадений для пути <tt class="doctest"><span class="pre">record/lx</span></tt> и получаем доступ к текстовому содержимому элемента, приводя его к нижнему регистру.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]
['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko',
'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', ..., 'kuvuto']</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>Давайте просматрим данные Toolbox в формате XML.  Метод <tt class="doctest"><span class="pre">write()</span></tt> из <tt class="doctest"><span class="pre">ElementTree</span></tt> ожидает файловый объект.  Мы обычно создаем один из них с помощью встроенной функции Python <tt class="doctest"><span class="pre">open()</span></tt>.  Для того, чтобы увидеть результат на экране, мы можем использовать специальный предустановленный файловый объект, называемый <tt class="doctest"><span class="pre">stdout</span></tt> <a class="reference internal" href="http://www.nltk.org/book/ch11.html#sys-stdout"><span id="ref-sys-stdout"><img class="callout" alt="[1]" src="http://www.nltk.org/book/callouts/callout1.gif"></span></a> (стандартный вывод), определенный в <tt class="doctest"><span class="pre">sys</span></tt> модуле  Python.</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; import sys
&gt;&gt;&gt; from nltk.util import elementtree_indent
&gt;&gt;&gt; from xml.etree.ElementTree import ElementTree
&gt;&gt;&gt; elementtree_indent(lexicon)
&gt;&gt;&gt; tree = ElementTree(lexicon[3])
&gt;&gt;&gt; tree.write(sys.stdout, encoding='unicode') 
&lt;record&gt;
  &lt;lx&gt;kaa&lt;/lx&gt;
  &lt;ps&gt;N&lt;/ps&gt;
  &lt;pt&gt;MASC&lt;/pt&gt;
  &lt;cl&gt;isi&lt;/cl&gt;
  &lt;ge&gt;cooking banana&lt;/ge&gt;
  &lt;tkp&gt;banana bilong kukim&lt;/tkp&gt;
  &lt;pt&gt;itoo&lt;/pt&gt;
  &lt;sf&gt;FLORA&lt;/sf&gt;
  &lt;dt&gt;12/Aug/2005&lt;/dt&gt;
  &lt;ex&gt;Taeavi iria kaa isi kovopaueva kaparapasia.&lt;/ex&gt;
  &lt;xp&gt;Taeavi i bin planim gaden banana bilong kukim tasol long paia.&lt;/xp&gt;
  &lt;xe&gt;Taeavi planted banana in order to cook it.&lt;/xe&gt;
&lt;/record&gt;</pre>
</td>
</tr></table></td></tr>
</table></div>
</div>
<div class="section" id="formatting-entries">
<h2>4.5 Форматирование записей</h2>
<p>Мы можем использовать ту же идею, которую мы видели выше, чтобы генерировать HTML-таблицы вместо обычного текста.
Это было бы полезно для публикации Toolbox лексикона в сети.
Данный код производит HTML-элементы <tt class="doctest"><span class="pre">&lt;table&gt;</span></tt>, <tt class="doctest"><span class="pre">&lt;tr&gt;</span></tt> (строки таблицы) и <tt class="doctest"><span class="pre">&lt;td&gt;</span></tt> (данные таблицы).</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; html = "&lt;table&gt;\n"
&gt;&gt;&gt; for entry in lexicon[70:80]:
...     lx = entry.findtext('lx')
...     ps = entry.findtext('ps')
...     ge = entry.findtext('ge')
...     html += "  &lt;tr&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;td&gt;%s&lt;/td&gt;&lt;/tr&gt;\n" % (lx, ps, ge)
&gt;&gt;&gt; html += "&lt;/table&gt;"
&gt;&gt;&gt; print(html)
&lt;table&gt;
  &lt;tr&gt;&lt;td&gt;kakae&lt;/td&gt;&lt;td&gt;???&lt;/td&gt;&lt;td&gt;small&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakae&lt;/td&gt;&lt;td&gt;CLASS&lt;/td&gt;&lt;td&gt;child&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakaevira&lt;/td&gt;&lt;td&gt;ADV&lt;/td&gt;&lt;td&gt;small-like&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakapikoa&lt;/td&gt;&lt;td&gt;???&lt;/td&gt;&lt;td&gt;small&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakapikoto&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;newborn baby&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakapu&lt;/td&gt;&lt;td&gt;V&lt;/td&gt;&lt;td&gt;place in sling for purpose of carrying&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakapua&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;sling for lifting&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakara&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;arm band&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;Kakarapaia&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;village name&lt;/td&gt;&lt;/tr&gt;
  &lt;tr&gt;&lt;td&gt;kakarau&lt;/td&gt;&lt;td&gt;N&lt;/td&gt;&lt;td&gt;frog&lt;/td&gt;&lt;/tr&gt;
&lt;/table&gt;</pre>
</td>
</tr></table></td></tr>
</table></div>
</div>
</div>
<div class="section" id="working-with-toolbox-data">
<span id="sec-working-with-toolbox-data"></span><h1>5 Работа с данными Toolbox</h1>
<p>Учитывая популярность Toolbox среди лингвистов, мы обсудим некоторые дополнительные методы работы с данными Toolbox.  Многие из методов, рассмотренных в предыдущих главах, например, подсчет, построение распределения частот, построение таблицы совместных вхождений, могут быть применены к содержанию записей Toolbox.  Например, мы можем тривиально вычислить среднее число полей для каждой записи:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; from nltk.corpus import toolbox
&gt;&gt;&gt; lexicon = toolbox.xml('rotokas.dic')
&gt;&gt;&gt; sum(len(entry) for entry in lexicon) / len(lexicon)
13.635...</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>В этом разделе мы рассмотрим две задачи, которые возникают в контексте документальной лингвистики, ни одна из которых не поддерживается программным обеспечением Toolbox.</p>
<div class="section" id="adding-a-field-to-each-entry">
<h2>5.1 Добавление поля для каждой записи</h2>
<p>Часто бывает удобно добавлять новые поля, которые получены автоматически из существующих.  Такие поля часто облегчают поиск и анализ.
Например, в <a class="reference internal" href="http://www.nltk.org/book/ch11.html#code-add-cv-field">5.1</a> мы определяем функцию <tt class="doctest"><span class="pre">cv()</span></tt>, которая ставит в соответствие строке гласных и согласных соответствующую CV последовательность, например строке <tt class="doctest"><span class="pre">kakapua</span></tt> соответствовала бы последовательность <tt class="doctest"><span class="pre">CVCVCVV</span></tt>. Это отображение имеет четыре шага.  Во-первых, строка преобразуется в нижний регистр, затем мы заменяем все не-буквенные символы <tt class="doctest"><span class="pre">[^a-z]</span></tt> на подчеркивание.
Далее, мы заменим все гласные на <tt class="doctest"><span class="pre">V</span></tt>.  И, наконец, любой символ, который не является <tt class="doctest"><span class="pre">V</span></tt> или подчеркиванием должен быть согласным, поэтому мы заменяем его на <tt class="doctest"><span class="pre">C</span></tt>. Теперь мы можем сканировать лексикон и добавить новое поле <tt class="doctest"><span class="pre">cv</span></tt> после каждого <tt class="doctest"><span class="pre">lx</span></tt> поля.
Листинг <a class="reference internal" href="http://www.nltk.org/book/ch11.html#code-add-cv-field">5.1</a> показывает, что это делает с конкретной записью; обратите внимание на последнюю строку вывода, которая показывает новое поле <tt class="doctest"><span class="pre">cv</span></tt>.</p>
<span class="target" id="code-add-cv-field"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
from xml.etree.ElementTree import SubElement

def cv(s):
    s = s.lower()
    s = re.sub(r'[^a-z]',     r'_', s)
    s = re.sub(r'[aeiou]',    r'V', s)
    s = re.sub(r'[^V_]',      r'C', s)
    return (s)

def add_cv_field(entry):
    for field in entry:
        if field.tag == 'lx':
            cv_field = SubElement(entry, 'cv')
            cv_field.text = cv(field.text)</pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; lexicon = toolbox.xml('rotokas.dic')
&gt;&gt;&gt; add_cv_field(lexicon[53])
&gt;&gt;&gt; print(nltk.toolbox.to_sfm_string(lexicon[53]))
\lx kaeviro
\ps V
\pt A
\ge lift off
\ge take off
\tkp go antap
\sc MOTION
\vx 1
\nt used to describe action of plane
\dt 03/Jun/2005
\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.
\xp Pita i go antap na lukim haus win i bagarapim.
\xe Peter went to look at the house that the wind destroyed.
\cv CVVCVCV</pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="http://www.nltk.org/book/pylisting/code_add_cv_field.py" type="text/x-python"><span class="caption-label">Пример 5.1 (code_add_cv_field.py)</span></a>: <span class="caption-label">Листинг 5.1:</span> Добавление нового поля <tt class="doctest"><span class="pre">cv</span></tt> к лексической записи</p></td></tr>
</table></div>
<div class="note">
<p class="first admonition-title">Замечание</p>
<p class="last">Если файл Toolbox постоянно обновляется, программа в листинге code_add_cv_field должна быть запущена более чем один раз.  Можно было бы модифицировать <tt class="doctest"><span class="pre">add_cv_field()</span></tt>, чтобы изменять содержимое существующей записи.  Тем не менее, более безопасной практикой является использование таких программ для создания обогащенных файлов с целью анализа данных без замены курируемых вручную исходных файлов.</p>
</div>
</div>
<div class="section" id="validating-a-toolbox-lexicon">
<h2>5.2 Проверка лексикона Toolbox</h2>
<p>Многие лексиконы в формате Toolbox не соответствуют какой-либо конкретной схеме.
Некоторые записи могут включать в себя дополнительные поля или могут упорядочивать существующие поля по-новому.
Ручная проверка тысяч лексических записей не применима.
Тем не менее, мы можем легко определить часто встречающиеся последовательности полей с помощью <tt class="doctest"><span class="pre">Counter</span></tt>:</p>
<div class="doctest">
<table border="0" cellpadding="0" cellspacing="0" class="doctest" width="95%">
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; from collections import Counter
&gt;&gt;&gt; field_sequences = Counter(':'.join(field.tag for field in entry) for entry in lexicon)
&gt;&gt;&gt; field_sequences.most_common()
[('lx:ps:pt:ge:tkp:dt:ex:xp:xe', 41), ('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe', 37),
('lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe', 27), ('lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe', 20), ...]</pre>
</td>
</tr></table></td></tr>
</table></div>
<p>После проверки этих последовательностей полей мы могли бы разработать контекстно свободную грамматику для лексических записей.  Грамматика в <a class="reference internal" href="http://www.nltk.org/book/ch11.html#code-toolbox-validation">5.2</a> использует формат CFG, который мы видели в <a class="reference external" href="http://www.nltk.org/book/ch08.html#chap-parse">8.</a>.  Такая грамматика моделирует неявно вложенную структуру записей Toolbox и строит древовидную структуру, в которой индивидуальные имена полей являются листьями дерева.  Наконец, мы перебираем записи и сообщаем об их соответствие грамматике, как показано в листинге <a class="reference internal" href="http://www.nltk.org/book/ch11.html#code-toolbox-validation">5.2</a>.
К тем полям, которые допускаются грамматикой, добавляется префикс <tt class="doctest"><span class="pre"><span class="pysrc-string">'+'</span></span></tt> <a class="reference internal" href="http://www.nltk.org/book/ch11.html#accepted-entries"><span id="ref-accepted-entries"><img class="callout" alt="[1]" src="http://www.nltk.org/book/callouts/callout1.gif"></span></a>, а к тем, которые отвергаются добавляется <tt class="doctest"><span class="pre"><span class="pysrc-string">'-'</span></span></tt> <a class="reference internal" href="http://www.nltk.org/book/ch11.html#rejected-entries"><span id="ref-rejected-entries"><img class="callout" alt="[2]" src="http://www.nltk.org/book/callouts/callout2.gif"></span></a>.
В процессе разработки такой грамматики удобно отфильтровать некоторые теги <a class="reference internal" href="http://www.nltk.org/book/ch11.html#ignored-tags"><span id="ref-ignored-tags"><img class="callout" alt="[3]" src="http://www.nltk.org/book/callouts/callout3.gif"></span></a>.</p>
<span class="target" id="code-toolbox-validation"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
grammar = nltk.CFG.fromstring('''
  S -&gt; Head PS Glosses Comment Date Sem_Field Examples
  Head -&gt; Lexeme Root
  Lexeme -&gt; "lx"
  Root -&gt; "rt" |
  PS -&gt; "ps"
  Glosses -&gt; Gloss Glosses |
  Gloss -&gt; "ge" | "tkp" | "eng"
  Date -&gt; "dt"
  Sem_Field -&gt; "sf"
  Examples -&gt; Example Ex_Pidgin Ex_English Examples |
  Example -&gt; "ex"
  Ex_Pidgin -&gt; "xp"
  Ex_English -&gt; "xe"
  Comment -&gt; "cmt" | "nt" |
  ''')

def validate_lexicon(grammar, lexicon, ignored_tags):
    rd_parser = nltk.RecursiveDescentParser(grammar)
    for entry in lexicon:
        marker_list = [field.tag for field in entry if field.tag not in ignored_tags]
        if list(rd_parser.parse(marker_list)):
            print("+", ':'.join(marker_list)) 
        else:
            print("-", ':'.join(marker_list))  <a name="rejected-entries"></a><a href="http://www.nltk.org/book/ch11.html#ref-rejected-entries"><img src="http://www.nltk.org/book/callouts/callout2.gif" alt="[2]" class="callout"></a></pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; lexicon = toolbox.xml('rotokas.dic')[10:20]
&gt;&gt;&gt; ignored_tags = ['arg', 'dcsv', 'pt', 'vx'] 
&gt;&gt;&gt; validate_lexicon(grammar, lexicon, ignored_tags)
- lx:ps:ge:tkp:sf:nt:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe
- lx:rt:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe
- lx:ps:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe
- lx:ps:ge:tkp:nt:sf:dt
- lx:ps:ge:tkp:dt:cmt:ex:xp:xe:ex:xp:xe
- lx:ps:ge:ge:ge:tkp:cmt:dt:ex:xp:xe
- lx:rt:ps:ge:ge:tkp:dt
- lx:rt:ps:ge:eng:eng:eng:ge:tkp:tkp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe:ex:xp:xe
- lx:rt:ps:ge:tkp:dt:ex:xp:xe
- lx:ps:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe</pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="http://www.nltk.org/book/pylisting/code_toolbox_validation.py" type="text/x-python"><span class="caption-label">Пример 5.2 (code_toolbox_validation.py)</span></a>: <span class="caption-label">Листинг 5.2:</span> Проверка Toolbox записей с использованием контекстно-свободной грамматики</p></td></tr>
</table></div>
<p>Другой подход заключается в использовании анализатора группировки (<a class="reference external" href="http://www.nltk.org/book/ch07.html#chap-chunk">7.</a>), так как он являются гораздо более эффективным при определении частичных структур и может сообщать о частичных структурах, которые были обнаружены.  В <a class="reference internal" href="http://www.nltk.org/book/ch11.html#code-chunk-toolbox">5.3</a> мы создали грамматику группировки для записей лексикона, а затем разобрали каждую запись.
Образец вывода этой программы показан в листинге <a class="reference internal" href="http://www.nltk.org/book/ch11.html#fig-iu-mien">5.4</a>.</p>
<span class="target" id="code-chunk-toolbox"></span><div class="pylisting">
<table border="0" cellpadding="0" cellspacing="0" class="pylisting" width="95%">
<tr><td class="codeblock">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_codeblock_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
grammar = r"""
      lexfunc: {&lt;lf&gt;(&lt;lv&gt;&lt;ln|le&gt;*)*}
      example: {&lt;rf|xv&gt;&lt;xn|xe&gt;*}
      sense:   {&lt;sn&gt;&lt;ps&gt;&lt;pn|gv|dv|gn|gp|dn|rn|ge|de|re&gt;*&lt;example&gt;*&lt;lexfunc&gt;*}
      record:   {&lt;lx&gt;&lt;hm&gt;&lt;sense&gt;+&lt;dt&gt;}
    """</pre>
</td>
</tr></table></td></tr>
<tr><td class="doctest">
<table border="0" cellpadding="0" cellspacing="0" width="100%">
<tr><td width="1" class="copybar" onclick="javascript:copy_doctest_to_clipboard(this.nextSibling);"> </td>
<td class="pysrc"><pre class="doctest">
&gt;&gt;&gt; from xml.etree.ElementTree import ElementTree
&gt;&gt;&gt; from nltk.toolbox import ToolboxData
&gt;&gt;&gt; db = ToolboxData()
&gt;&gt;&gt; db.open(nltk.data.find('corpora/toolbox/iu_mien_samp.db'))
&gt;&gt;&gt; lexicon = db.parse(grammar, encoding='utf8')
&gt;&gt;&gt; tree = ElementTree(lexicon)
&gt;&gt;&gt; with open("iu_mien_samp.xml", "wb") as output:
...     tree.write(output)</pre>
</td>
</tr></table></td></tr>
<tr><td class="caption"><p class="caption"><a class="reference external" href="http://www.nltk.org/book/pylisting/code_chunk_toolbox.py" type="text/x-python"><span class="caption-label">Пример 5.3 (code_chunk_toolbox.py)</span></a>: <span class="caption-label">Листинг 5.3</span>: Группировка лексикона Toolbox: грамматика группировки, описывающая структуру записей лексикона для <span class="emphasis">Iu Mien</span>, одного из языка Китая.</p></td></tr>
</table></div>
<span class="target" id="fig-iu-mien"></span><div class="figure" id="fig-iu-mien">
<img alt="../images/iu-mien.png" src="http://www.nltk.org/images/iu-mien.png" style="width:537.5px;height:429.0px">
<p class="caption"><span class="caption-label">Рисунок 5.4:</span> XML-представление лексической записи, полученное в результате разбора группировки записи Toolbox</p>
</div>
</div>
</div>
<div class="section" id="describing-language-resources-using-olac-metadata">
<h1>6 Описание языковых ресурсов с помощью метаданных OLAC</h1>
<p>Члены сообщества NLP имеют общую потребность в обнаружении языковых ресурсов с высокой точностью и охватом.
Решение, которое было разработано сообществом цифровых библиотек (the Digital Libraries community) включает агрегацию метаданных.</p>
<div class="section" id="what-is-metadata">
<h2>6.1 Что такое метаданные?</h2>
<p>Самое простое определение метаданных "структурированные данные о данных". Метаданные - это описательная информацию об объекте или ресурсе, будь то физическом или электронном. Хотя термин метаданные сам по себе является относительно новым, лежащие в его основе концепции использовались с тех пор, как были организованы сборники информации.
Библиотечные каталоги представляют собой устоявшийся тип метаданных; они служили в качестве средств управления сбором и обнаружения ресурсов в течение многих десятилетий. Метаданные могут быть получены либо "вручную" или сгенерированы автоматически с помощью программного обеспечения.</p>
<p>Дублинская Инициатива Базовых Метаданных возникла в 1995 году для разработки конвенций поиска ресурсов в Интернете.
Элементы дублинских базовых метаданных представляют собой широкий, междисциплинарный консенсус относительно основного набора элементов, которые могут быть полезны на практике для поддержки поиска ресурсов.
Дублинская база состоит из 15 элементов метаданных, каждый из которых является необязательным и повторяемым: название, создатель, тема, описание, издатель, участники, дата, тип, формат, идентификатор, источник, язык, отношение, охват, права.
Этот набор метаданных может быть использован для описания ресурсов, которые существуют в цифровых или традиционных форматах.</p>
<p>Инициатива открытых архивов (The Open Archives initiative (OAI)) предоставляет общую структуру цифровых хранилищ научных материалов независимо от их типа, включая документы, данные, программное обеспечение, записи, физические артефакты, цифровые суррогаты и так далее.
Каждый репозиторий состоит из доступного сетевого сервера, предлагающий публичный доступ к архивным элементам.  Каждый элемент имеет уникальный идентификатор и связан с записью дублинских базовых метаданных (и, возможно, с дополнительными записями в других форматах).  OAI определяет протокол для услуг поиска метаданных, которые "собирают" содержимое хранилищ.</p>
</div>
<div class="section" id="olac-open-language-archives-community">
<h2>6.2 OLAC: Сообщество открытых языковых архивов</h2>
<p>Сообщество открытых языковых архивов (Open Language Archives Community (OLAC)) - это международное партнерство учреждений и людей, которые создают всемирную виртуальную библиотеку языковых ресурсов посредством: (i) разработки консенсуса по наилучшей текущей практике цифрового архивирования языковых ресурсов и (ii) разработки сети взаимодействующих хранилищ и услуг по хранению и получению доступа к таким ресурсам.
Домашняя страница OLAC в сети: <tt class="doctest"><span class="pre">http://www.language-archives.org/</span></tt>.</p>
<p>Метаданные OLAC являются стандартом для описания языковых ресурсов.
Единообразное описание разных хранилищ обеспечивается путем ограничения значений некоторых элементов метаданных использованием терминов из контролируемых словарей. Метаданные OLAC могут быть использованы для описания данных и инструментов, в физических и цифровых форматах.
Метаданные OLAC расширяют метаданные дублинский базовый набор, широко принятый стандарт для описания ресурсов всех типов.
К этому основному набору OLAC добавляет дескрипторы для покрытия основных свойств языковых ресурсов, таких как язык предмета и языковый тип.  Вот пример полной записи OLAC:</p>
<pre class="literal-block">
&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;olac:olac xmlns:olac="http://www.language-archives.org/OLAC/1.1/"
           xmlns="http://purl.org/dc/elements/1.1/"
           xmlns:dcterms="http://purl.org/dc/terms/"
           xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
           xsi:schemaLocation="http://www.language-archives.org/OLAC/1.1/
                http://www.language-archives.org/OLAC/1.1/olac.xsd"&gt;
  &lt;title&gt;A grammar of Kayardild. With comparative notes on Tangkic.&lt;/title&gt;
  &lt;creator&gt;Evans, Nicholas D.&lt;/creator&gt;
  &lt;subject&gt;Kayardild grammar&lt;/subject&gt;
  &lt;subject xsi:type="olac:language" olac:code="gyd"&gt;Kayardild&lt;/subject&gt;
  &lt;language xsi:type="olac:language" olac:code="en"&gt;English&lt;/language&gt;
  &lt;description&gt;Kayardild Grammar (ISBN 3110127954)&lt;/description&gt;
  &lt;publisher&gt;Berlin - Mouton de Gruyter&lt;/publisher&gt;
  &lt;contributor xsi:type="olac:role" olac:code="author"&gt;Nicholas Evans&lt;/contributor&gt;
  &lt;format&gt;hardcover, 837 pages&lt;/format&gt;
  &lt;relation&gt;related to ISBN 0646119966&lt;/relation&gt;
  &lt;coverage&gt;Australia&lt;/coverage&gt;
  &lt;type xsi:type="olac:linguistic-type" olac:code="language_description"/&gt;
  &lt;type xsi:type="dcterms:DCMIType"&gt;Text&lt;/type&gt;
&lt;/olac:olac&gt;
</pre>
<p>Участвующие языковые архивы публикуют свои каталоги в формате XML, и эти записи регулярно "собираются" сервисами OLAC с использованием протокола OAI.  В дополнение к этой программной инфраструктуре OLAC задокументировала ряд лучших практик для описания языковых ресурсов в рамках процесса, который включал расширенные консультации с сообществом языковых ресурсов (например, см. <tt class="doctest"><span class="pre">http://www.language-archives.org/REC/bpr.html)</span></tt>.</p>
<p>Можно осуществлять поиск по OLAC репозиториям с помощью поисковой системы на сайте OLAC.
Поиск фразы "German lexicon" находит следующие ресурсы среди прочего:</p>
<ul class="simple">
<li>CALLHOME Немецкий Lexicon <tt class="doctest"><span class="pre">http://www.language-archives.org/item/oai:www.ldc.upenn.edu:LDC97L18</span></tt></li>
<li>MULTILEX многоязычный лексикон <tt class="doctest"><span class="pre">http://www.language-archives.org/item/oai:elra.icp.inpg.fr:M0001</span></tt></li>
<li>Slelex фонетический лексикон Siemens
<tt class="doctest"><span class="pre">http://www.language-archives.org/item/oai:elra.icp.inpg.fr:S0048</span></tt></li>
</ul>
<p>Поиск "Korean" находит newswire corpus, a treebank, a lexicon, a child-language corpus, подстрочно комментированные тексты.  Он также находит программное обеспечение, в том числе синтаксический анализатор и морфологический анализатор.</p>
<p>Заметим, что приведенные выше URL-адреса включают подстроку формы: <tt class="doctest"><span class="pre">oai:www.ldc.upenn.edu:LDC97L18</span></tt>.  Это идентификатор OAI, использующий схему URI , зарегистрированную в ICANN <em>(Интернет корпорация по присвоению имен и номеров)</em>.
Эти идентификаторы имеют формат <tt class="doctest"><span class="pre">oai:</span></tt><em>archive</em>:<em>local_id</em>, где <tt class="doctest"><span class="pre">oai</span></tt> это имя схемы URI, <em>archive</em> представляет собой идентификатор архива, такой как <tt class="doctest"><span class="pre">www.ldc.upenn.edu</span></tt>, а <em>local_id</em> - это идентификатор ресурса, назначенный архиву, например, <tt class="doctest"><span class="pre">LDC97L18</span></tt>.</p>
<p>Для данного идентификатора OAI данного OLAC ресурса можно получить полную XML-запись для данного ресурса по ссылке следующего вида:</p>
<pre class="literal-block">
http://www.language-archives.org/static-records/oai:archive:local_id
</pre>
</div>
<div class="section" id="disseminating-language-resources">
<h2>6.3 Распространение языковых ресурсов</h2>
<p>Консорциум лингвистических данных хранит <a name="nltk_data_repository_index_term"></a><span class="termdef">Репозиторий данных NLTK</span>, архив открытого доступа, куда члены сообщества могут загружать корпусы и сохраненные модели.  Эти ресурсы могут быть легко получены с помощью инструмента загрузки NLTK.</p>
</div>
</div>
<div class="section" id="summary">
<h1>6 Резюме</h1>
<ul class="simple">
<li>Основные типы данных, присутствующие в большинстве корпусов, -  аннотированные тексты и словари.  Тексты имеют временную структуру, в то время как лексиконы имеют структуру записи.</li>
<li>Жизненный цикл корпуса включает в себя сбор данных, аннотацию, контроль качества и публикации.  Жизненный цикл продолжается после публикации по мере того, как корпус изменяется и обогащается в ходе исследования.</li>
<li>Разработка полезного корпуса требует соблюдения баланса между представительной выборкой использования языка и достаточным количеством материала из любого одного источника или жанра; перемножение измерений вариации, как правило, не представляется возможным из-за ресурсных ограничений.</li>
<li>XML предоставляет полезный формат для хранения и обмена лингвистическими данными, но не предоставляет никаких готовых решений широко распространенных проблем моделирования данных.</li>
<li>Формат Toolbox широко используется в проектах по документированию языков; мы можем писать программы для поддержки курирования файлов Toolbox и конвертировать их в XML.</li>
<li>Сообщество открытых языковых архивов (OLAC) обеспечивает инфраструктуру для документирования и обнаружения языковых ресурсов.</li>
</ul>
</div>
<div class="section" id="further-reading">
<span id="sec-further-reading-data"></span><h1>7 Дополнительные материалы</h1>
<p>Дополнительные материалы для этой главы размещены на странице <tt class="doctest"><span class="pre">http://nltk.org/</span></tt>, в том числе ссылки на свободно доступные ресурсы в сети.</p>
<p>Основными источниками языковых корпусов являются <em>/Linguistic Data Consortium</em> и <em>European Language Resources Agency</em>, оба с обширными онлайн-каталогами.
Подробности относительно основных корпусов, упомянутых в главе, доступны в следующих работах: 
American National Corpus <a class="reference external" href="http://www.nltk.org/book/bibliography.html#reppen2005anc" id="id2">(Reppen, Ide, &amp; Suderman, 2005)</a>, 
British National Corpus <a class="reference external" href="http://www.nltk.org/book/bibliography.html#bnc1999" id="id3">({BNC}, 1999)</a>, 
Thesaurus Linguae Graecae <a class="reference external" href="http://www.nltk.org/book/bibliography.html#tlg1999" id="id4">({TLG}, 1999)</a>,
Child Language Data Exchange System (CHILDES) <a class="reference external" href="http://www.nltk.org/book/bibliography.html#macwhinney1995childes" id="id5">(MacWhinney, 1995)</a>,
TIMIT <a class="reference external" href="http://www.nltk.org/book/bibliography.html#garofolo1986timit" id="id6">(S., Lamel, &amp; William, 1986)</a>.</p>
<p>Двумя специальными группами Ассоциации по вычислительной лингвистике, которые организуют регулярные семинары, с опубликованными материалами являются SIGWAC, которая способствует использованию сети в качестве корпуса и спонсировала задачу CLEANEVAL для удаления HTML-разметки, и SIGANN, которая способствует усилиям, направленным на взаимодействие лингвистических аннотаций.</p>
<p>Полная информация о формате данных Toolbox предоставляются с дистрибутивом <a class="reference external" href="http://www.nltk.org/book/bibliography.html#buseman1996shoebox" id="id7">(Buseman, Buseman, &amp; Early, 1996)</a> и с последним дистрибутивом, свободно доступом на <tt class="doctest"><span class="pre">http://www.sil.org/computing/toolbox/</span></tt>. Инструкции по процессу построения Toolbox лексикона см. на <tt class="doctest"><span class="pre">http://www.sil.org/computing/ddp/</span></tt>. Дополнительные примеры программ для Toolbox документированы в <a class="reference external" href="http://www.nltk.org/book/bibliography.html#bird1999nels" id="id8">(Tamanji, Hirotani, &amp; Hall, 1999)</a>, <a class="reference external" href="http://www.nltk.org/book/bibliography.html#robinson2007toolbox" id="id9">(Робинсон, Aumann, &amp; Bird, 2007)</a>.
Для управления лингвистическими данными доступны десятки других инструментов, некоторые из которых обследованы в <a class="reference external" href="http://www.nltk.org/book/bibliography.html#bird2003portability" id="id10">(Bird &amp; Simons, 2003)</a>.
Смотрите также труды мастерских "LaTeCH" по языковым технологиям для данных культурного наследия.</p>
<p>Есть много отличных ресурсов по XML (например , <tt class="doctest"><span class="pre">http://zvon.org/</span></tt>) и по написанию программ Python для работы с XML.  Многие редакторы имеют режимы XML.
Форматы XML для лексической информации включают OLIF <tt class="doctest"><span class="pre">http://www.olif.net/</span></tt> и LIFT <tt class="doctest"><span class="pre">http://code.google.com/p/lift-standard/</span></tt>.</p>
<p>Для ознакомления с обследованием программного обеспечения лингвистической аннотации см. <em>Linguistic Annotation Page</em> на <tt class="doctest"><span class="pre">http://www.ldc.upenn.edu/annotation/</span></tt>. Первоначально внешняя аннотация была предложена в <a class="reference external" href="http://www.nltk.org/book/bibliography.html#thompson1997standoff" id="id11">(Thompson &amp; McKelvie, 1997)</a>.
Абстрактная модель данных для лингвистических аннотаций, называемая "аннотационные графы", была предложена в <a class="reference external" href="http://www.nltk.org/book/bibliography.html#bird2001annotation" id="id12">(Bird &amp; Liberman, 2001)</a>.
Онтология общего назначения для лингвистического описания (GOLD) описана на <tt class="doctest"><span class="pre">http://www.linguistics-ontology.org/</span></tt>.</p>
<p>Для ознакомления с руководством по планированию и построению корпуса, см. <a class="reference external" href="http://www.nltk.org/book/bibliography.html#meyer2002" id="id13">(Meyer, 2002)</a>, <a class="reference external" href="http://www.nltk.org/book/bibliography.html#farghaly2003" id="id14">(Farghaly, 2003)</a>. Более подробная информация о методах оценки согласия между аннотаторами доступна в работах <a class="reference external" href="http://www.nltk.org/book/bibliography.html#artsteinpoesio2008" id="id15">(Artstein &amp; Poesio, 2008)</a>, <a class="reference external" href="http://www.nltk.org/book/bibliography.html#pevzner2002windowdiff" id="id16">(Pevzner &amp; Hearst, 2002)</a>.</p>
<p>Данные Ротокас были предоставлены ​​Стюартом Робинсоном, а данные Iu Mien были предоставлены ​​Грегом Ауманном.</p>
<p>Для получения дополнительной информации о Сообществе открытых языковых архивов посетите <tt class="doctest"><span class="pre">http://www.language-archives.org/</span></tt> или посмотрите <a class="reference external" href="http://www.nltk.org/book/bibliography.html#simonsbird2003llc" id="id17">(Simons &amp; Bird, 2003)</a>.</p>
</div>
<div class="section" id="exercises">
<h1>9 Упражнения</h1>
<ol class="arabic">
<li><p class="first">◑ В <a class="reference internal" href="http://www.nltk.org/book/ch11.html#code-add-cv-field">5.1</a> новое поле появилось в нижней части записи.  Изменить эту программу так , чтобы она вставляет новый подэлемент сразу после поля <tt class="doctest"><span class="pre">лк.</span></tt>  (Подсказка: создать новое поле <tt class="doctest"><span class="pre">резюме</span></tt> с помощью <tt class="doctest"><span class="pre">элемента ( <span class="pysrc-string">"резюме"),</span></span></tt> присвоить текстовое значение для него, а затем использовать метод <tt class="doctest"><span class="pre">вставки ()</span></tt> родительского элемента.)</p>
</li>
<li><p class="first">◑ Написать функцию, которая удаляет заданное поле из лексической записи.
(Мы могли бы использовать эту функцию, чтобы дезинфицировать наши лексические данные, прежде чем дать его другим, например, путем удаления полей, содержащих нерелевантные или неопределенного содержания.)</p>
</li>
<li><p class="first">◑ Напишите программу , которая сканирует файл словаря HTML , чтобы найти записи , имеющие незаконный неполный из речи поля, и сообщает <em>заглавное слово</em> для каждой записи.</p>
</li>
<li><p class="first">◑ Напишите программу , чтобы найти какие - либо части речи <tt class="doctest"><span class="pre">(пс</span></tt> поле) , которое произошло менее чем в десять раз.  Возможно, эти опечатки?</p>
</li>
<li><p class="first">◑ Мы увидели способ обнаружения случаев целого слова удвоением.
Написать функцию, чтобы найти слова, которые могут содержать частичную редупликацию.  Используйте метод <tt class="doctest"><span class="pre">re.search (),</span></tt> а также следующее регулярное выражение: <tt class="doctest"><span class="pre">(..{0}{1}!={/1}{/0}</span></tt></p>
</li>
<li><p class="first">◑ Мы увидели способ добавления поля <tt class="doctest"><span class="pre">резюме.</span></tt>  Существует интересная проблема с сохранением этого последнюю дату , когда кто - то изменяет содержимое поля <tt class="doctest"><span class="pre">лк</span></tt> , на котором она основана.  Напишите версию этой программы , чтобы добавить поле <tt class="doctest"><span class="pre">резюме,</span></tt> замена любого существующего поля <tt class="doctest"><span class="pre">резюме.</span></tt></p>
</li>
<li><p class="first">◑ Написать функцию , чтобы добавить новый <tt class="doctest"><span class="pre">SYL</span></tt> поле , что дает подсчет количества слогов в слове.</p>
</li>
<li><p class="first">◑ Написать функцию, которая отображает полную запись для лексемы.
Когда лексема неправильно пишется он должен отображать запись для наиболее аналогичным образом прописана лексемы.</p>
</li>
<li><p class="first">◑ Написать функцию , которая принимает лексикон и обнаруживает , какие пары последовательных полей являются наиболее частыми (например , <tt class="doctest"><span class="pre">пс</span></tt> часто сопровождается <tt class="doctest"><span class="pre">пт).</span></tt>
(Это может помочь нам обнаружить некоторые структуры лексической записи.)</p>
</li>
<li><p class="first">◑ Создание таблицы с помощью офисного программного обеспечения, содержащий одну лексическую запись на строку, состоящую из заглавного слова, часть речи, и блеск.  Сохраните таблицу в формате CSV.  Написать код Python , чтобы прочитать файл CSV и распечатать его в формате Toolbox, используя <tt class="doctest"><span class="pre">лк</span></tt> для заглавного слова, <tt class="doctest"><span class="pre">пс</span></tt> для части речи и <tt class="doctest"><span class="pre">ОЛ</span></tt> для блеска.</p>
</li>
<li><p class="first">◑ Индекс слова пьесы Шекспира, с помощью <tt class="doctest"><span class="pre">NLTK.Индекс.</span></tt>
Полученная структура данных должна позволять поиск по отдельным словам , такие как <span class="example">музыка,</span> возвращая список ссылок на акты, сцены и выступлений, формы <tt class="doctest"><span class="pre">[(3, 2, 9), (5, 1, 23), ...]</span></tt> , где <tt class="doctest"><span class="pre">(3, 2, 9)</span></tt> указывает на то Акт 3 Сцена 2 Speech 9.</p>
</li>
<li><p class="first">◑ Построить условное распределение частот , который регистрирует длину слова для каждого слова в <span class="emphasis">Венецианском купце,</span> кондиционированной по имени персонажа, например , <tt class="doctest"><span class="pre">кфд [ <span class="pysrc-string">'Порция']</span> [12]</span></tt> даст нам число выступлений Порции , состоящий из 12 слова.</p>
</li>
<li><p class="first">★ Получить сравнительный список слов в формате CSV, и написать программу, которая печатает эти родственные слова, имеющие от изменений расстояния, по меньшей мере, трех друг от друга.</p>
</li>
<li><p class="first">★ Построить индекс тех лексем, которые появляются в примеры предложений.
Предположим , что лексема для данной записи является <em>ш.</em>
Затем добавить один перекрестных ссылок поля <tt class="doctest"><span class="pre">XRF</span></tt> к этому входу, ссылаясь на заглавных других записей , имеющих пример предложения , содержащие <em>W.</em>  Сделайте это для всех записей и сохранить результат в виде файла формата инструментов.</p>
</li>
<li><p class="first">◑ Написать рекурсивную функцию для получения представления XML для дерева, с нетерминалов, представленных в виде элементов XML и листьев, представленных в виде текстового содержания, например:</p>
<pre class="literal-block">
&lt;S&gt; &lt;тип NP = "SBJ"&gt; &lt;NP&gt; &lt;ННП&gt; Pierre &lt;/ ННП&gt; &lt;ННП&gt; Vinken &lt;/ ННП&gt; &lt;/ NP&gt; &lt;COMMA&gt;, &lt;/ COMMA&gt;
</pre>
</li>
</ol>
<!-- Footer to be used in all chapters -->
<div class="admonition-about-this-document admonition">
<p class="first admonition-title">Об этом документе ...</p>
<p>Обновлялся для NLTK 3.0.
Это глава из книги <em>Обработка естественного языка с помощью Python</em> написанной <a class="reference external" href="http://estive.net/">Стивеном Бердом</a> , <a class="reference external" href="http://homepages.inf.ed.ac.uk/ewan/">Эваном Клайном</a> и <a class="reference external" href="http://ed.loper.org/">Эдвардом Лопером</a> , Copyright © 2014 авторов.
Он распространяется с <em>Набором инструментов для естественного языка</em> <tt class="doctest"><span class="pre">[http://nltk.org/],</span></tt> версия 3.0 в соответствии с условиями <em>Creative Commons Attribution-Noncommercial-No Derivative Works 3.0 Лицензии Соединенных Штатов</em> [ <a class="reference external" href="http://creativecommons.org/licenses/by-nc-nd/3.0/us/">http://creativecommons.org/licenses/by-nc-nd/3.0/us/</a>].</p>
<p class="last">Этот документ был построен на ср 1 июля 2015 12:30:05 AEST</p>
</div>
</div>
</div>
</body>
</html>